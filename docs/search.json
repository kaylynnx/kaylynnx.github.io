[
  {
    "objectID": "posts/LimitsQuantitativeApproach/Essay.html",
    "href": "posts/LimitsQuantitativeApproach/Essay.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "As long as humans have existed, technology has been a powerful force. From the wheel to running water to the computer, technology has aided our day-to-day lives. In modern-day technology, since the mid-1900s, technology has expanded past new inventions to more quantitative innovations. The generation of new phones and computers has become relatively stagnant, but the usage of phones and computers to aid our day-to-day lives has exploded. Scientists and businessmen created Machine Learning, more colloquially known as Artificial Intelligence. Artificial Intelligence has been displayed in movies such as Robocop or the recently released horror movie, Megan. These movies depict a universe in which Artificial Intelligence is self-aware and relatively human. While largely exaggerated, we have created machines to learn from data, hence why it is called Machine Learning. Given some historical data or a collection of current data, the machines can generate some sort of conclusion. \n\n\nMachine Learning is used in the finance industry to uncover credit card fraud, make predictions about creditworthiness, and identify trends in the stock market (Ethical Implications of Bias in Machine Learning). Machine learning has been incredibly productive and helpful to provide important advances in health care and treatment decisions (Ethical Implications of Bias in Machine Learning). The criminal justice system is using machine learning to predict crime hotspots and recidivism rates (Ethical Implications of Bias in Machine Learning). AI and Machine learning has been increasingly widespread. They have been helpful in all aspects of our lives. But with all great innovations, there are unintended consequences. One of the unintended consequences is bias. What is bias? When we think of bias or fairness in a social, political, or economic sense, bias is defined as prejudice against one person, group, or thing in a way classified as ‘unfair’. Essentially, bias in an algorithmic sense, which we coin algorithmic bias, is very similar. Most researchers in this field define algorithmic bias differently. Nevertheless, “most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status” (Bias in Machine Learning – What is it Good For). What does this mean exactly? Well, bias in Machine Learning isn’t so black or white. Instead, the authors of Bias in Machine Learning found that biases can be organized into three distinct categories: “a biased world, data generation, and learning” (Bias in Machine Learning – What is it Good For). Within a biased world, bias may propagate through what people would refer to as historical bias. According to the Australian Human Rights Commission, “historical bias arises when the data used to train an AI system no longer accurately reflects the current reality” (Australian Human Rights Commission). Many obsolete data contain outdated language or ideals. For instance, when we looked at the Titanic data set, we see that sex contained a binary male or female. However, more up-to-date data would also consider those who are non-binary. Secondly, in the data generation category, they found five different sources of bias, including specification bias and inherited bias. Specification bias occurs when a potential independent component is excluded from the general model, resulting in a biased estimate of the coefficients. Inherited bias refers to the underlying assumptions that skew viewpoints and data. Finally, in the learning category, a way bias propagates is through inductive bias. The inductive bias of an algorithm is the set of assumptions the learner uses to predict outputs of given inputs that it has not encountered. That list is just a small, but widespread, list of ways bias presents itself in Machine Learning and algorithms.\n\n\nThese biases are reflected in the usage of algorithms in all of the examples I listed above. In finance, machine learning has been used to make predictions about creditworthiness. But what isn’t discussed is how minorities and underrepresented groups are hypothesized to have lower creditworthiness. As a result, it is harder for them to get loans, buy homes, and make investments. Machine learning has also made large advancements in healthcare. But for a while, women were given worse treatment than men because predictions were directed toward men. And POC was given worse treatment than white people because predictions and calculations were made specifically for white people. Machine learning was used in the criminal justice system to predict crime and recidivism rates, but the algorithm was so skewed that the algorithm targeted black individuals in black communities. It turned out that black individuals were given harsher sentences than white individuals for the same crime. But there are also other discriminatory algorithms. A study found that “Amazon’s recruitment tool, which produced AI-based recommendations that significantly favored men over women for technical jobs” (Questioning Racial and Gender Bias in AI-based Recommendations: Do Espoused National Cultural Values Matter?). \n\nStudying discrimination and prejudice in these algorithms, many researchers have concluded that these quantitative methods are incredibly problematic. One of those researchers is Arvind Narayanan. In his speech, The Limits of the Quantitative Approach to Discrimination, he argues that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Arvind Narayanan). While this is a strong statement, Arvind backs up this claim with strong evidence. In his speech, he talks about the practice of quantitative methods. More specifically, he talks about the usage of the null hypothesis. When studying racism, for instance, the null hypothesis is that there is no racism. Similarly, when studying any form of prejudice, the null hypothesis is that there is no prejudice. This is problematic because “baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (Arvind Narayanan). It’s problematic that the null hypothesis is that there is no discrimination because when there is no significant evidence that discrimination exists, people verify and justify the null hypothesis, thus seeing the status quo as something without flaws or issues. And “when researchers pick the null hypothesis on autopilot, mimicking what’s been done before, they are often oblivious to the fact that their choice has enormous normative significance” (Arvind Narayanan). There is also the issue with data. Often the data we use are snapshots from a day or a short period. But snapshots also lose people’s lived experiences because it is not recorded in the data. So, snapshots “frame discrimination as happening at discrete moments in time rather than encoded into the way that our institutions are designed” (Arvind Narayanan). In other words, the data doesn’t paint a holistic image, ignoring systematic and structural discrimination. Another problematic aspect of these quantitative methods is who produces the data. Usually, larger companies and organizations, both of which are arguably biased, gather data. So, “when companies are in control of producing data, they have simple ways of affecting the conclusions that are drawn by controlling which data are collected or released” (Arvind Narayanan). \n\nThese sentiments by Arvind are echoed through other research papers. Dr. Alex Hanna from Harvard University, similar to Arvind, explains that AI research is supporting ‘bad markets’ for a lack of better words. Machine learning, algorithms, and AI are controlled by “large tech companies, elite universities, or specialty labs like Open AI or Anthropic which have these really big VC term sheets and are doing things which they consider to be general-purpose AI or something of that nature” (Dr. Alex Hanna). But the reason this is bad is that it doesn’t allow researchers to be independent or community focused. Instead, it supports a capitalistic society such as corporate uses, supporting their businesses, or directed towards military and security purposes. Thus Dr. Alex Hanna concludes that AI was created for negative reasons. As an example of this, Clearview AI, a large institution, created a widely known and used facial recognition tool for law enforcement. And ShotSpotter detects things like gunshots. These tools are designed to micromanage low-income communities and communities of color. Furthermore, she explains that a lot of the problem lies in the data. She explains that the data “does not provide the full story” (Dr. Alex Hanna). \n\nAdrienne Yapo and Joseph Weiss of Bentley University expressed similar disdain toward algorithms and machine learning. They suggested that the most significant issue is the “black box” secrecy behind the machine learning algorithms. In other words, the algorithms are not transparent. Why? “For-profit companies that produce these algorithms do not release the criteria and calculations behind the formulas” (Yapo and Weiss). Furthermore, at times the algorithms become so complex that understanding the formulas is extremely difficult. Therefore, since the algorithms are created by humans, “they inevitably – and often unconsciously – reflect societal values, biases, and discriminatory practices” (Yapo and Weiss). Ultimately, while Yapo and Weiss find that AI can be helpful at times, reformation is needed. Inclusivity and awareness of the ethical risks and complications are crucial to the design of AI to ensure that individuals are treated fairly. \n\nTechnology and machine learning have come a long way. They have supported individuals in our day-to-day lives, making them important. However, the current usages of these algorithms and the foundations on which they lie are problematic. I, along with many other researchers and individuals, wouldn’t go as far as to say that the harms outweigh the benefits. There needs change and an ethical discussion, but these algorithms can still be helpful. \n\nSources:\n    \nhttps://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf\n\nhttps://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&context=hicss-51\n\nhttps://arxiv.org/pdf/2004.00686.pdf \n\nhttps://link.springer.com/article/10.1007/s10796-021-10156-2\n\nhttps://www.sir.advancedleadership.harvard.edu/articles/understanding-gender-and-racial-bias-in-ai\n\nhttps://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems"
  },
  {
    "objectID": "posts/Perceptron/index.html",
    "href": "posts/Perceptron/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "p = perceptron.Perceptron()\n\np.fit(X, y, 1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2\n\n\n# https://github.com/kaylynnx/kaylynnx.github.io"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/PenguinPost/Penguin.html",
    "href": "posts/PenguinPost/Penguin.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "train_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n#train.head()\n\n\ntrain = train[train.Sex != \".\"]\n\n\n# sns.set_theme(style=\"whitegrid\", palette = \"muted\")\n\n# ax = sns.swarmplot(data = train, x = \"Body Mass (g)\", y = \"Sex\", hue = \"Species\")\n# ax.set(ylabel = \"\")\n# plt.show()\n# sns.pairplot(train, hue= \"Species\")\n# swarm plot\nsns.scatterplot(x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\", data = train)\nplt.show()\n\n\n\n\n\ndef read_data(url):\n  df = pd.read_csv(url)\n  y = df[\"Body Mass (g)\"]\n  X = df.drop([\"Body Mass (g)\", \"Species\"], axis = 1)\n  return df, X, y\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\n\ndf_train, X_train, y_train = read_data(train_url)\n\n\n#y_train.head()\ntrain = pd.get_dummies(train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n      Sex_MALE\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      7.96621\n      -25.69327\n      NaN\n      0\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      NaN\n      0\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      NaN\n      1\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      NaN\n      1\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      NaN\n      1\n    \n  \n\n\n\n\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      7.96621\n      -25.69327\n      NaN\n      0\n      0\n    \n    \n      1\n      PAL0708\n      22\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      7.63220\n      -25.46569\n      NaN\n      1\n      0\n    \n    \n      2\n      PAL0910\n      124\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      9.59462\n      -25.42621\n      NaN\n      0\n      1\n    \n    \n      3\n      PAL0910\n      146\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      9.22033\n      -26.03442\n      NaN\n      0\n      1\n    \n    \n      4\n      PAL0708\n      24\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      9.28153\n      -24.97134\n      NaN\n      0\n      1\n    \n  \n\n\n\n\n\n# train = pd.get_dummies(train, columns = ['Sex'], drop_first = \"if_binary\")\n\ntrain.groupby('Species')[['Body Mass (g)', 'Sex_MALE']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Sex_MALE\n    \n    \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.09\n      118\n      0.47\n      118\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.86\n      56\n      0.48\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5121.97\n      100\n      0.54\n      100\n    \n  \n\n\n\n\n\nWhen looking at the species of penguins and their body mass, I can see that the Gentoo penguins are, on average, much larger in body mass than the chinstrap penguins and the adelie penguins.\n\nsns.scatterplot(data=train, x='Body Mass (g)', y='Flipper Length (mm)')\n\n<AxesSubplot: xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\n\ngrouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\ngrouped\n\n/var/folders/k5/sv5thph578dff6sqh3h3z2kw0000gn/T/ipykernel_25399/3046570246.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  grouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\n\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      median\n      std\n      mean\n      median\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.094017\n      3650.0\n      455.209898\n      189.965812\n      190.0\n      6.678493\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.857143\n      3687.5\n      404.876925\n      195.464286\n      195.0\n      7.032300\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5121.969697\n      5100.0\n      511.653391\n      217.656566\n      217.0\n      6.438344\n    \n  \n\n\n\n\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      7.96621\n      -25.69327\n      NaN\n      0\n      0\n    \n    \n      1\n      PAL0708\n      22\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      7.63220\n      -25.46569\n      NaN\n      1\n      0\n    \n    \n      2\n      PAL0910\n      124\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      9.59462\n      -25.42621\n      NaN\n      0\n      1\n    \n    \n      3\n      PAL0910\n      146\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      9.22033\n      -26.03442\n      NaN\n      0\n      1\n    \n    \n      4\n      PAL0708\n      24\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      9.28153\n      -24.97134\n      NaN\n      0\n      1\n    \n  \n\n\n\n\nDescription of the scatterplot and the table above\n\n# #for loop for features\n\n# # from sklearn.linear_model import LogisticRegression\n\n# # LR = LogisticRegression()\n# # LR.fit(X_train, y_train)\n# # LR.score(X_train, y_train)\n# from sklearn.linear_model import LogisticRegression\n\n# # this counts as 3 features because the two Clutch Completion \n# # columns are transformations of a single original measurement. \n# # you should find a way to automatically select some better columns\n# # as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Culmen Depth (mm)\", \"Sex_MALE\"]\n\n# LR = LogisticRegression()\n# LR.fit(X_train[cols], y_train)\n# LR.score(X_train[cols], y_train)\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\npair = combinations(all_quant_cols, 3)\ncols = qual_cols + list(pair)\nX_train_sub = X_train[cols]\n\n/Users/kaylynnxia/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/common.py:245: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  result = np.asarray(values, dtype=dtype)\n\n\nKeyError: \"[('Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)')] not in index\"\n\n\n\nbest_cols = []\nbest_acc = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 3):\n        cols = qual_cols + list(pair)\n        X_train_sub = X_train[cols]\n        X_test_sub = X_test[cols]\n        le = LabelEncoder()\n        y_train_enc = le.fit_transform(y_train)\n        y_test_enc = le.transform(y_test)\n        clf = RandomForestClassifier(n_estimators=100, random_state=0)\n        clf.fit(X_train_sub, y_train_enc)\n        acc = clf.score(X_test_sub, y_test_enc)\n        if acc == 1:\n            best_cols = cols\n            best_acc = acc\n            break\n    if best_acc == 1:\n        break\n\nNameError: name 'LabelEncoder' is not defined\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    # \n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\n\ndf_test, X_test, y_test = read_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Species\"], drop_first=\"if_binary\")\n\nKeyError: \"None of [Index(['Species'], dtype='object')] are in the [columns]\"\n\n\n\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score\n\n# # select the relevant columns\n# qual_col = [\"Clutch Completion\", \"Sex\"]\n# quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n# # create the feature and target arrays\n# X = train[quant_cols + [qual_col]].values\n# y = train['Species'].values\n\n# # split the data into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# def train_and_evaluate(X_train, y_train, X_test, y_test):\n#     # train the model on the training set\n#     clf = RandomForestClassifier(n_estimators=100, random_state=42)\n#     clf.fit(X_train, y_train)\n    \n#     # make predictions on the test set\n#     y_pred = clf.predict(X_test)\n    \n#     # calculate the accuracy of the model\n#     accuracy = accuracy_score(y_test, y_pred)\n    \n#     return accuracy\n\n\n# from itertools import combinations\n\n# best_accuracy = 0\n# best_features = None\n\n# for qual in [qual_col]:\n#     qual_cols = [col for col in train.columns if qual in col]\n#     for pair in combinations(quant_cols, 2):\n#         cols = qual_cols + list(pair)\n#         X_train_subset = X_train[:, [i for i, col in enumerate(train.columns) if col in cols]]\n#         X_test_subset = X_test[:, [i for i, col in enumerate(train.columns) if col in cols]]\n#         accuracy = train_and_evaluate(X_train_subset, y_train, X_test_subset, y_test)\n#         if accuracy > best_accuracy:\n#             best_accuracy = accuracy\n#             best_features = cols\n\n# print(\"Best features:\", best_features)\n# print(\"Best accuracy:\", best_accuracy)"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\n\nLR = LogisticRegression()\nLR.fit(X,y)\nplot_decision_regions(X,y,clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X,y)}\")\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\nLR2 = LogisticRegression()\n\nX_ = X**2\nLR2 = LogisticRegression();\nLR2.fit(X_, y)\nplot_decision_regions(X_, y, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_, y)}\")\n#\n#\n#\n#"
  },
  {
    "objectID": "Gradient.html",
    "href": "Gradient.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from solutions.logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nModuleNotFoundError: No module named 'solutions'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Perceptron.html",
    "href": "Perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "p = perceptron.Perceptron()\n\np.fit(X, y, 1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2"
  }
]