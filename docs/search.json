[
  {
    "objectID": "posts/LinearRegression/LinearRegression.html",
    "href": "posts/LinearRegression/LinearRegression.html",
    "title": "Hello Blog - Linear Regression",
    "section": "",
    "text": "Part 1: Implementing Linear Regression in 2 ways!\nThe first cells below were given in the instructions.\nFor the Linear Regression class, I know that we needed two fit function, fit_gradient and fit_analytic. We also need a prediction function and a score function. The class also has an instance of weights. For my gradient descent implementation of linear regression (fit_gradient), I first pad X_train. Then I initialize the weights. Then, I run a loop that performs num_iterations of gradient descent updates. This learns the optimal weights that minimize the square error loss. For my analytic implementation of Linear Regression (fit_analytic), I first pad X_train like I did in gradient descent. Differently, I initialize the weights using the equation. Everything else is the same.\nFurthermore, I have a function (pred) that computes a predicted output y_pred. It is padded X dot weights. Lastly, I have a score function (coefficient determination). The score is calculated as 1 - (the residual sum of squares/the total sum of squares).\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\n\n\n\n\n\nclass LinearRegression:\n    def __init__(self, learning_rate=0.01, num_iterations=5000):\n        self.learning_rate = learning_rate\n        self.num_iterations = num_iterations\n        self.weights = None\n        self.r2_scores_train = []  # List to store training scores\n        self.r2_scores_val = []  # List to store validation scores\n    \n    def pad(self, X):\n        return np.append(X, np.ones((X.shape[0], 1)), axis=1)\n    \n    def fit_gradient(self, X_train, y_train):\n        X_padded = self.pad(X_train)\n        n_samples, n_features = X_padded.shape\n        self.weights = np.zeros(n_features)\n        \n        for _ in range(self.num_iterations):\n            gradients = (2 / n_samples) * X_padded.T.dot(X_padded.dot(self.weights) - y_train)\n            self.weights -= self.learning_rate * gradients\n            \n            # Calculate scores and store them\n            y_train_pred = X_padded.dot(self.weights)\n            r2_train = self.coeff_determination(y_train, y_train_pred)\n            self.r2_scores_train.append(r2_train)\n            \n            y_val_pred = self.predict(X_val)\n            r2_val = self.coeff_determination(y_val, y_val_pred)\n            self.r2_scores_val.append(r2_val)\n    \n    def fit_analytic(self, X_train, y_train):\n        X_padded = self.pad(X_train)\n        self.weights = np.linalg.inv(X_padded.T @ X_padded) @ X_padded.T @ y_train\n        \n        # Calculate scores and store them\n        y_train_pred = X_padded.dot(self.weights)\n        r2_train = self.coeff_determination(y_train, y_train_pred)\n        self.r2_scores_train.append(r2_train)\n        \n        y_val_pred = self.predict(X_val)\n        r2_val = self.coeff_determination(y_val, y_val_pred)\n        self.r2_scores_val.append(r2_val)\n        \n    def predict(self, X):\n        X_padded = self.pad(X)\n        y_pred = X_padded.dot(self.weights)\n        return y_pred\n    \n    def coeff_determination(self, y_true, y_pred):\n        mean_y = np.mean(y_true)\n        ss_total = np.sum((y_true - mean_y) ** 2)\n        ss_residual = np.sum((y_true - y_pred) ** 2)\n        coeff_determination = 1 - (ss_residual / ss_total)\n        return coeff_determination\n\n    def w(self):\n        return self.weights\n\nWhen I train my data with the gradient fit, I can see that training score is 74% and the validation score is 65%. This is not terrible.\nWe can further look at the scores over time. We can see that the at one iteration pretty early on, the algorithm is able to arrive at “the best” score.\nWe can also print out the weights, as denoted by model.w().\n\n# Generate the data\nX_train, y_train, X_val, y_val = LR_data()\n\n# Create an instance of LinearRegression\nmodel = LinearRegression(learning_rate=0.1, num_iterations=1000)\n\n# Train the linear regression model\nmodel.fit_gradient(X_train, y_train)\n\n# Make predictions on the training and validation data\ny_train_pred = model.predict(X_train)\ny_val_pred = model.predict(X_val)\n\n# Calculate the score for training and validation data\nr2_train = model.coeff_determination(y_train, y_train_pred)\nr2_val = model.coeff_determination(y_val, y_val_pred)\n\nprint(\"Training Score:\", r2_train)\nprint(\"Validation Score:\", r2_val)\n\n# Visualize the results\nplt.scatter(X_val, y_val, color='b', label='Actual')\nplt.scatter(X_val, y_val_pred, color='r', label='Predicted')\nplt.legend()\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.show()\n\nTraining Score: 0.7396452171477115\nValidation Score: 0.6509874662490212\n\n\n\n\n\n\nmodel.w()\n\narray([0.59643518, 0.82674881])\n\n\n\n# Plot the scores over time\nplt.plot(range(1, len(model.r2_scores_val) + 1), model.r2_scores_val)\nplt.xlabel('Iteration')\nplt.ylabel('Score')\nplt.title('Score over Time')\nplt.show()\n\n\n\n\nWe can run the same data but with our analytic fit function. When we do this, we see that we will get a training score of 80.0% and a validation score of 79.9%. This is a very solid score!\n\n# Generate the data\nX_train, y_train, X_val, y_val = LR_data()\n\n# Create an instance of LinearRegression\nmodel = LinearRegression(learning_rate=0.1, num_iterations=1000)\n\n# Train the linear regression model\nmodel.fit_analytic(X_train, y_train)\n\n# Make predictions on the training and validation data\ny_train_pred = model.predict(X_train)\ny_val_pred = model.predict(X_val)\n\n# Calculate the score for training and validation data\nr2_train = model.coeff_determination(y_train, y_train_pred)\nr2_val = model.coeff_determination(y_val, y_val_pred)\n\nprint(\"Training Score:\", r2_train)\nprint(\"Validation Score:\", r2_val)\n\n# Visualize the results\nplt.scatter(X_val, y_val, color='b', label='Actual')\nplt.scatter(X_val, y_val_pred, color='r', label='Predicted')\nplt.legend()\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.show()\n\nTraining Score: 0.8002254869378185\nValidation Score: 0.7993011810100353\n\n\n\n\n\nLooking at the two implementations of linear regression, it is hard to determine which one is the best implementation. When refreshing the code, we arrive at different scores each time - sometimes great and sometimes not so great! Both code gets us to an output.\nPart 3: Experimenting\nMoving on from part 1, we can use the code above to run some experiments. We can look at the effect of the number of features on training and validation scores. We can see below that as we increase the number of features, both the training and validation scores get extremely bad. This makes sense because as the number of features increases, we increase the possibility for overfitting, which would make the scores worse.\n\n# Generate the data\nn_train = 100\nn_val = 100\nnoise = 0.1\n\n# Define the range of features\np_features_range = range(1, n_train)\n\n# Lists to store scores\ntrain_scores = []\nval_scores = []\n\n# Iterate over the range of features\nfor p_features in p_features_range:\n    # Generate data for current p_features\n    X_train, y_train, X_val, y_val = LR_data(n_train=n_train, n_val=n_val, p_features=p_features, noise=noise)\n    \n    # Create an instance of LinearRegression\n    model = LinearRegression(learning_rate=0.1, num_iterations=100)\n    \n    # Train the linear regression model\n    model.fit_gradient(X_train, y_train)\n    \n    # Make predictions on the training and validation data\n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    # Calculate the score\n    r2_train = model.coeff_determination(y_train, y_train_pred)\n    r2_val = model.coeff_determination(y_val, y_val_pred)\n    \n    # Append scores to the lists\n    train_scores.append(r2_train)\n    val_scores.append(r2_val)\n\n# Plot the training and validation scores\nplt.plot(p_features_range, train_scores, label='Training Score')\nplt.plot(p_features_range, val_scores, label='Validation Score')\nplt.xlabel('Number of Features')\nplt.ylabel('Score')\nplt.title('Effect of Number of Features on Training and Validation Scores')\nplt.legend()\nplt.show()\n\n\n\n\nPart 4: Lasso\n\n#Lasso\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.01)\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nL.score(X_val, y_val)\n\n0.6672328921448816\n\n\nLasso is another algorithm that we can use. When we LASSO to train our data, we get a score of 66.7%.\nLet’s take a closer look at the experimentation with LASSO. When we increase the number of features, we can see that for both LASSO and Linear Regression, the score gets worse. However, the score under LASSO gets significantly worse. This is because LASSO prevents overfitting, whereas, there is nothing preventing the Linear Regression models from overfitting. We can also see with LASSO that as we increase the alpha values, the scores get better.\n\n#Experimenting\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\n\n# Generate the data\nn_train = 100\nn_val = 100\nnoise = 0.1\n\n# Define the range of features\np_features_range = range(1, n_train+20)\n\n# Define the regularization strengths to test\nalpha_values = [0.001, 0.01, 0.1]\n\n# Lists to store scores\nlinear_reg_scores = []\nlasso_scores = []\n\n# Iterate over the range of features\nfor p_features in p_features_range:\n    # Generate data for current p_features\n    X_train, y_train, X_val, y_val = LR_data(n_train=n_train, n_val=n_val, p_features=p_features, noise=noise)\n    \n    # Linear Regression\n    linear_reg_model = LinearRegression()\n    linear_reg_model.fit_gradient(X_train, y_train)\n    y_val_pred_linear_reg = linear_reg_model.predict(X_val)\n    linear_reg_score = r2_score(y_val, y_val_pred_linear_reg)\n    linear_reg_scores.append(linear_reg_score)\n    \n    # LASSO\n    lasso_scores_for_alpha = []\n    for alpha in alpha_values:\n        lasso_model = Lasso(alpha=alpha)\n        lasso_model.fit(X_train, y_train)\n        y_val_pred_lasso = lasso_model.predict(X_val)\n        lasso_score = r2_score(y_val, y_val_pred_lasso)\n        lasso_scores_for_alpha.append(lasso_score)\n    lasso_scores.append(lasso_scores_for_alpha)\n\n# Plot the validation scores\nplt.figure(figsize=(10, 6))\n\n# Linear Regression\nplt.plot(p_features_range, linear_reg_scores, label='Linear Regression')\n\n# LASSO\nfor i, alpha in enumerate(alpha_values):\n    lasso_scores_for_alpha = [scores[i] for scores in lasso_scores]\n    plt.plot(p_features_range, lasso_scores_for_alpha, label=f'LASSO (alpha={alpha})')\n\nplt.xlabel('Number of Features')\nplt.ylabel('Score')\nplt.title('Validation Scores Comparison: Linear Regression vs LASSO')\nplt.legend(loc='lower right')\nplt.show()\n\n/Users/kaylynnxia/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.278e-01, tolerance: 6.612e-02\n  model = cd_fast.enet_coordinate_descent("
  },
  {
    "objectID": "posts/LimitsQuantitativeApproach/Essay.html",
    "href": "posts/LimitsQuantitativeApproach/Essay.html",
    "title": "Hello Blog",
    "section": "",
    "text": "Machine Learning is used in the finance industry to uncover credit card fraud, make predictions about creditworthiness, and identify trends in the stock market (Yapo and Weiss (2018)). Machine learning has been incredibly productive and helpful to provide important advances in health care and treatment decisions (Yapo and Weiss (2018)). The criminal justice system is using machine learning to predict crime hotspots and recidivism rates (Yapo and Weiss (2018)). AI and Machine learning has been increasingly widespread. They have been helpful in all aspects of our lives. But with all great innovations, there are unintended consequences. One of the unintended consequences is bias. What is bias? When we think of bias or fairness in a social, political, or economic sense, bias is defined as prejudice against one person, group, or thing in a way classified as ‘unfair’. Essentially, bias in an algorithmic sense, which we coin algorithmic bias, is very similar. Most researchers in this field define algorithmic bias differently. Nevertheless, “most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status” (Hellstrom_Bensch_Dignum_2020). What does this mean exactly? Well, bias in Machine Learning isn’t so black or white. Instead, the authors of Bias in Machine Learning found that biases can be organized into three distinct categories: “a biased world, data generation, and learning” (Hellstrom_Bensch_Dignum_2020). Within a biased world, bias may propagate through what people would refer to as historical bias. According to the Australian Human Rights Commission, “historical bias arises when the data used to train an AI system no longer accurately reflects the current reality” ( (1970)). Many obsolete data contain outdated language or ideals. For instance, when we looked at the Titanic data set, we see that sex contained a binary male or female. However, more up-to-date data would also consider those who are non-binary. Secondly, in the data generation category, they found five different sources of bias, including specification bias and inherited bias. Specification bias occurs when a potential independent component is excluded from the general model, resulting in a biased estimate of the coefficients. Inherited bias refers to the underlying assumptions that skew viewpoints and data. Finally, in the learning category, a way bias propagates is through inductive bias. The inductive bias of an algorithm is the set of assumptions the learner uses to predict outputs of given inputs that it has not encountered. That list is just a small, but widespread, list of ways bias presents itself in Machine Learning and algorithms.\nStudying discrimination and prejudice in these algorithms, many researchers have concluded that these quantitative methods are incredibly problematic. One of those researchers is Arvind Narayanan. In his speech, The Limits of the Quantitative Approach to Discrimination, he argues that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan (n.d.)). While this is a strong statement, Narayanan backs up this claim with strong evidence. In his speech, he talks about the practice of quantitative methods. More specifically, he talks about the usage of the null hypothesis. When studying racism, for instance, the null hypothesis is that there is no racism. Similarly, when studying any form of prejudice, the null hypothesis is that there is no prejudice. This is problematic because “baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (Narayanan (n.d.)). It’s problematic that the null hypothesis is that there is no discrimination because when there is no significant evidence that discrimination exists, people verify and justify the null hypothesis, thus seeing the status quo as something without flaws or issues. And “when researchers pick the null hypothesis on autopilot, mimicking what’s been done before, they are often oblivious to the fact that their choice has enormous normative significance” (Narayanan (n.d.)). There is also the issue with data. Often the data we use are snapshots from a day or a short period. But snapshots also lose people’s lived experiences because it is not recorded in the data. So, snapshots “frame discrimination as happening at discrete moments in time rather than encoded into the way that our institutions are designed” (Narayanan (n.d.)). In other words, the data doesn’t paint a holistic image, ignoring systematic and structural discrimination. Another problematic aspect of these quantitative methods is who produces the data. Usually, larger companies and organizations, both of which are arguably biased, gather data. So, “when companies are in control of producing data, they have simple ways of affecting the conclusions that are drawn by controlling which data are collected or released” (Narayanan (n.d.)). Therefore, Narayanan would argue that the current quantitative methods for assessing discrimination and bias justify the status quo.\nLet’s look a little bit more closely at the quantitative methods that assess discrimination and bias. In this article, Weight Bias Among Health Professionals Specializing in Obesity, researchers looked at “anti-fat bias in health professionals” (Schwartz et al. (2003)). Anti-fat has been a more modern way of thinking. Lately, our culture has praised the ultra-skinny but shames those who are bigger in size. Among those who echo these bias sentiments are unfortunately health care professionals. While not all of them are anti-fat, there remain some who are, which decreases the standard of care. This can ruin people’s lives. In this study, the researchers found that “the obesity specialists in the study exhibited a significant implicit anti-fat bias” (Schwartz et al. (2003)). Often times, the healthcare professionals would assign the words “lazy, stupid, and worthless” to the overweight people. However, they explain that “people who work directly with obese patients exhibited less anti-fat bias” (Schwartz et al. (2003)). Looking at bias in this context is very important because there are real implications associated with anti-fat biases. How do these factors impact healthcare standards? How does this affect people’s perception of the doctors and do those perceptions affect the rate at which people are seeking medical attention? Investigating these biases are very beneficial. It’s important that people, especially healthcare workers, understand their implicit biases so that they can better treat patients from different backgrounds.\nFurthermore, we can look closer at algorithmic auditing. These audits are used to help “expose systematic biases embedded in software platforms” (Raji and Buolamwini (2019)). There are many audits that researchers use to evaluate discrimination and biases in algorithms. And these targeted algorithmic audits “provide one mechanism to incentivize corporations to address the algorithmic bias present in data-centric technologies that continue to play an integral role in daily life, from governing access to information and economic opportunities to influencing personal freedoms” (Raji and Buolamwini (2019)). These algorithmic audits are incredibly helpful to review potential discrimination found in large corporations and algorithms. For instance, there was a study that found that “Amazon’s recruitment tool, which produced AI-based recommendations that significantly favored men over women for technical jobs” (Gupta, Parra, and Dennehy (2021)). An algorithmic audit on this would have been very informative and helpful. These audits focus on “user awareness of algorithmic bias” and “evaluate the impact of bias on user behavior and outcomes” (Raji and Buolamwini (2019)). By spreading awareness of these widespread algorithms, built by monopolized tech corporations, people can better understand the harms surrounding the algorithms. More specifically, to audit commercial systems, such as Amazon’s recruitment tool, researchers will use a ‘black box audit’, where “the direct or indirect influence of input features on classifier accuracy or outcomes is inferred through the evaluation of a curated benchmark” (Raji and Buolamwini (2019)). When using this audit, the vendor names are kept anonymous, and the scope is scaled down to a single named target. This means that it rarely provokes public pressure and reduces the impetus for corporate reactions. Regardless, we are able to better ourselves by understanding the implications of big tech algorithms.\nSimilar to the black box audit, there is also the gender shades study. It is an “external and multi-target black box audit of commercial machine learning” (Raji and Buolamwini (2019)). Within this audit, they test “attributes of gender, reduced to the binary categories of male or female, as well as binary Fitzpatrick score, a numerical classification schema for human skin type, evaluated by a dermatologist, and grouped into classes of lighter and darker skin types” (Raji and Buolamwini (2019)). This audit seems problematic. By reducing gender to a binary male or female, we ignore those who do not fit into either category such as binary or transgender people. Furthermore, there are similar issues presented when we look at skin color as ‘lighter’ or ‘darker’. However, when using the gender shades study to evaluate companies, the results were very promising. The researchers from this study found that “by highlighting the issue of classification performance disparities and amplifying public awareness, the study was able to motivate companies to prioritize the issue and yield significant improvements within 7 months” (Raji and Buolamwini (2019)).\nLooking at the sources, it seems like the quantitative methods of assessing discrimination and bias partially follows the status quo. However, I would argue that they do more good than harm. It is important to consider the intentions behind these quantitative methods. Take the Gender Shades Study, for instance. It is highly problematic that they label gender as a binary female and male and classify skin color as lighter or darker. However as a result, we are bringing public awareness to an issue. People can then make decisions after reading and understanding the data.\nTechnology and machine learning have come a long way. They have supported individuals in our day-to-day lives, making them important. However, the current usages of these algorithms and the foundations on which they lie are problematic. When looking at the algorithms, people use quantitative methods to assess the discrimination and bias. I, along with many other researchers and individuals, wouldn’t go as far as to say that the harms outweigh the benefits. There needs change and an ethical discussion, but these quantitative methods can still be helpful.\n\n\n\n\nReferences\n\n1970. The Australian Human Rights Commission. https://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems.\n\n\nGupta, Manjul, Carlos M. Parra, and Denis Dennehy. 2021. “Questioning Racial and Gender Bias in AI-Based Recommendations: Do Espoused National Cultural Values Matter? - Information Systems Frontiers.” SpringerLink. Springer US. https://link.springer.com/article/10.1007/s10796-021-10156-2.\n\n\nNarayanan, Arvind. n.d. “The Limits of the Quantitative Approach to Discrimination - 2022 James Baldwin Lecture, Princeton University.” Princeton University. The Trustees of Princeton University. https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/.\n\n\nRaji, Inioluwa Deborah, and Joy Buolamwini. 2019. “Actionable Auditing.” Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. https://doi.org/10.1145/3306618.3314244.\n\n\nSchwartz, Marlene B., Heather O’Neal Chambliss, Kelly D. Brownell, Steven N. Blair, and Charles Billington. 2003. “Weight Bias Among Health Professionals Specializing in Obesity.” Obesity Research 11 (9): 1033–39. https://doi.org/10.1038/oby.2003.142.\n\n\nYapo, Adrienne, and Joseph Weiss. 2018. Ethical Implications of Bias in Machine Learning. https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&amp;context=hicss-51."
  },
  {
    "objectID": "posts/Perceptron/Perceptron.html",
    "href": "posts/Perceptron/Perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "**Quick walk through of my algorithm:\nFirst, as given to us, I start with making my blobs. I initially start with n = 100. But when I was doing my experimentation, I would change n as I changed the number of features. I wanted to have 50 samples for each feature. I also would remove the centers when I increased the number of features.\nThen I created my Perceptron class. In the indicator function I created, I returned 1 if X > 0, otherwise 0. Then I created the fit function. First, I set the weights to random numbers and I set the training score to 0. Within the for loop, I did a few calculations. I started by calculating the prediction error by doing the dot product between X and w (which are the weights plus b). I think compare y and y_pred. Based on that, I update all components of w tilde (w and b). I also update the score and the history of the scores.\nThat brings me to how I calculated the score. I score is the average of getting it right (1) vs. getting it wrong (0). I couldn’t do the score function without a prediction function. The prediction function is just the dot product of X and w. I also have the function w, which just returns the weights, and the function, history, which just turns the history of scores.\nFinally, I plot everything.\n**Experimentation:\nThe first set of experimentation that I did was changing the learning rates. Included in my fit function within my Perceptron class, I set the learning rates between 0.001 and 1. I noticed that when I changed my learning rate to 0.001, the perceptron algorithm learned slowly, much slower. However, when I set the perceptron algorithm to 1, the algorithm learned extremely fast, and I couldn’t see the progression of the accuracy. So, the sweet spot was setting the learning rate to 0.1.\nI also increeased the number of features I had to 10. When I did this, I noticed that the accuracy rate toggled in the 30’s (30% or so). This suggests that as I add features, my perceptron learns less accurately. Furthermore, when I increase the number of interations to 2,000 and maintained the number of features at 10, the accurac rate jumped to the 60’s (60% or so). Despite have a larger number of iterations, having a linearly separable graph gets more difficult. With 2 features, 1000 iterations, and a linearly separable graph, I noticed that my algorithm will always converge and produce an accuracy of 1. However, with 2 features, 1000 iterations, and a non-linearly separable graph, my algorithm never converges. The accuracy rate, at one point will stay stagnant.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n#n = (p_features - 1) * 50\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n# X, y = make_blobs(n_samples = n, n_features = p_features - 1)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nclass Perceptron:\n    def __init__(self):\n        self.weights = None\n        self.training_score = None\n        self.training_history = []\n    \n    def indicator(self, X):\n        return np.where(X > 0, 1, -1)\n    \n    def fit(self, X, y, num_iterations=1000, learning_rate=0.01):\n        self.weights = np.random.randn(X.shape[1] + 1)  # initialize weights randomly\n        print(f'Initial Weights: {self.weights}')\n\n        self.training_score = 0.0  # initialize score\n        for i in range(num_iterations):\n            y_pred = self.indicator(np.dot(X, self.weights[:-1]) + self.weights[-1])  # predict on all samples\n            # update weights based on prediction error\n            misclassified = y * y_pred <= 0\n            self.weights[:-1] += learning_rate * np.dot(X.T, y * misclassified)\n            self.weights[-1] += learning_rate * np.sum(y * misclassified)\n            # calculate score and update history\n            self.training_score = np.mean(y == y_pred)\n            self.training_history.append(self.training_score)\n\n    \n    def predict(self, X):\n        # add bias term to input\n        X = np.c_[X, np.ones(X.shape[0])]\n        return self.indicator(np.dot(X, self.weights))\n    \n    def w(self):\n        return self.weights\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        return np.mean(y == y_pred)\n    \n    def history(self):\n        return self.training_history\n\n\np = Perceptron()\np.fit(X,y, num_iterations = 1000)\n\nprint(f'Weights: {p.w()}')\nprint(f'Last 10 Training History: {p.history()[-10:]}')\nprint(f'Score: {p.score(X,y)}')\n\nplt.plot(p.history())\n\nInitial Weights: [ 1.67834508 -0.76553614  0.04903625]\nWeights: [1.47322257 0.1682865  0.73903625]\nLast 10 Training History: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\nScore: 0.5\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w(), -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nTesting my perceptron algorithm on linearly seperable data, we can see that my perceptron algorithm is able to correctly separate the two sets of points, as seen below:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate linearly separable data\nnp.random.seed(0)\nX = np.random.randn(100, 2)\ny = np.where(X[:,0] + X[:,1] > 0, 1, -1)\n\n# Fit the perceptron algorithm to the data\np = Perceptron()\np.fit(X, y)\n\n# Plot the decision boundary\nfig = plt.scatter(X[:,0], X[:,1], c=y)\nfig = draw_line(p.w(), -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.show()\n\nInitial Weights: [-0.36918184 -0.23937918  1.0996596 ]\n\n\n\n\n\n**Runtime:\nI believe that the runtime of one iteration of perceptron depends on the number of features but not on the number of data points. When we do the perceptron algorithm, there is one main dot product and then we update the weights. Going through this math, I believe that the runtime would be O(p). There would be n multiplications and n additions. And because it is only one iteration, the number of n does not matter."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/TimnitGebru/TimnitGebru.html",
    "href": "posts/TimnitGebru/TimnitGebru.html",
    "title": "Hello Blog",
    "section": "",
    "text": "On April 24th, Timnit Gebru will be visiting Middlebury College to give a presentation on bias and social impacts of AI. Born and raised in Ethiopia, Timnit is an American computer scientist who works on algorithmic biases and data mining. She has been a vocal advocate for diversity in technology, she is the co-founder of Black in AI, a community of Black researchers working in AI, and she is the founder of the Distributed Artificial Intelligence Research Institute (DAIR). Timnit had a rough childhood ranging from her father’s death, feeling the Eritrean-Ethiopian war, being denied entry to the United States, and facing systematic racism in her early years of living in the United States. Her experiences led her to ethics in technology. In 2001, Timnit was accepted at Stanford University where she earned her Bachelor of Science and Master of Science degrees in electrical engineering and her PhD in computer vision. Shortly after her completing of her PhD, she joined Google where she co-led a team on the ethics of artificial intelligence. She studied the implications of artificial intelligence, looking to improve the ability of technology to do social good. However, just two years into her role, in December 2020, Gebru controversially departed from Google. Timnit had published a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” that covered the risks of large language models. In December 2020, Gebru’s employment ended after high Google managers asked her to either withdraw the paper or remove the names of all the Google employees from the paper. Gebru requested insight into their decision and warned that non-compliance would result in her departure. Google terminated her employment immediately, stating they were accepting her resignation. Aside from this controversy, Timnit has been widely recognized. She was named one of the World’s 50 Greatest Leaders by Fortune, won the 2019 AI Innovations Award in the category AI for Good for highlighting the significant problem of algorithmic bias in facial recognition, one of Nature’s ten people who shaped science in 2021, and one of Time’s most influential people in 2022.\nTimnit Gebru discussed “faception” in the 2020 Fairness Accountability Transparency and Ethics in Computer Vision lecture. What is faception? She explains that it is the “first to-technology and first-to-market with proprietary computer vision and machine learning technology for profiling people and revealing their personality based only on their facial image”. It other words, it is an algorithm that can determine your IQ or see whether or not you are a terrorist based on one facial image. This type of technology is incredibly harmful because it incorrectly categorizes people, often in a negative way, and it negatively targets minorities and those in underrepresented groups. It’s technology like this that Gebru warns us about. It’s not just faception now. For example, the Hire Vue video intelligence that detects your verbal and non-verbal cues to determine whether you are qualified for a job. It determines whether you are ‘happy’ or ‘smart’, which are all very subjective descriptions. The Baltimore police also uses extensive facial recognition in an unconstitutional way. They used facial recognition to surveil people at protests and rallies – activities protected by the First Amendment. But doing such discourages political participation. They used facial recognition on social media photos to identify people at the Freddie Gray protests and target them for unnecessary arrests. Digging a little bit deeper into facial recognition, Gebru discusses the usage of facial recognition in big tech corporations such as IBM and Google. She found that there were high error rates for darker-skinned women and light-skinned people. The reason for such was a lack of diversity in data sets that people use. A lot of data comes from exclusively western settings. Similarly, a lot of experiments exclusively use men. For example, in clinical trials, medicine is tested on men only. As a result, women tend to suffer more. The same is true with safety testing using dummies. A lot of the times, testing, especially for vehicle safety, use male dummies. In the real world, women are disproportionately harmed. There is also a lot of harm for transgender individuals, especially in gender recognition. There are issues with misgendering, and facial recognition also assumes that gender is binary. On the bright side, there have been an increasing amount of pushback. Gebru is a part of an organization called Black in AI, which is “a place for sharing ideas, fostering collaboration, and discussing initiatives to increase the presence of Black people in the field of AI”. People are also combatting facial recognition through interesting makeup that fools facial recognition, fashion that people wear to fool facial recognition, and systems of refusal in engaging with this technology.\nAs used today, computer vision is harmful to underrepresented minorities and women because large corporations and privileged groups are using computer vision to gain power, profits, or control populations.\nA question I have for Timnit: In your 2020 lecture you explain that “there are very few people who are black, especially in the Computer Vision community”. There are also very few women. Being a woman of color in the computer vision community, have you felt imposter syndrome? If so, how do you handle such feelings?   Part 2:\nWe listened to Timnit Gebru’s lecture on “Eugenics and the Promise of Utopia through Artificial General Intelligence”. In her discussion, she starts by explaining how many people are not getting rich from AI. Contrary to popular beliefs, creating well-functioning and modern AI require exploited workers. Their job is to supply data without getting compensation. For instance, Kenyan workers are paid less than $2 per hour to moderate explicit and violent content to help produce work like ChatGPT. So, when wealthy individuals talk about the benefits of AI, they fail to acknowledge the harms to underprivileged people and exploited employees.\nFurthermore, Gebru connects AGI to Eugenics. Starting with the discussion of AGI, Gebru explains how the definition of AGI is not well-defined. When people attempt to discuss AGI, they explain AGI as an autonomous system that can outperform and out-think humans. It almost sounds like God. And this way of thinking is rooted in the 20th century Anglo-American thinking, such as Eugenics.\nWhen thinking about Eugenics, many people connect the movement to Nazis and believe that eugenics disappeared at the end of World War II. However, eugenics continue. We can split eugenics into three waves, but Gebru focuses on the first two. In the first wave, the goal of eugenics was to “improve the human stock through negative eugenics”. We can define negative eugenics as the attempt to get rid of undesirable traits because it would help improve the overall human race. Undesirable traits include idiocrasy, specific skin colors, disabilities, etc. This caused discrimination against under-represented groups. And we saw this will the mass genocide of Jews during World War II. After the first wave of eugenics, which died at the end of WWII, we entered the second wave of eugenics. The goal of the second wave was to improve the human stock through positive eugenics. This includes giving people the ability to design their children or encouraging people with desirable traits to reproduce more. But within Eugenics, we see people strive to “transcend humanism”, emerge with technology, and explode with intelligence.\nWe see this goal arise with AGI as well. The goal of many people is to enhance the human race and grow beyond what humans are capable of. And this leads us to the TESCREAL bundle. AGI can lead to utopia, or it can lead to an apocalypse. Starting with AGI utopia, we saw the idea of utopia emerge during the first and second wave eugenics. But in short, if we produce the necessary technology, we can bloom and thrive. On the other hand, technology can go wrong and lead to an apocalypse. People often oversimplify technology and the future of technology by classifying it as either a utopia or an apocalypse. In reality, AGI has started a race to the bottom. Every company feels like they have to create a certain model that can do everything. And this leads to a lot of centralization of power. This makes it difficult for small organizations to get funding for projects and products. Also, it is often the case that larger companies do subpar work compared to smaller companies. This debunks the idea of AGI Utopia. In terms of AGI Apocalypse, Gebru explains that by talking about at apocalypse, which is much more likely than a utopia, we are detracting accountability from the corporations that are building the machines. Instead, we are placing the blame on the machines itself.\nGiven this talk, I find that there is accountability that needs to happen, especially within large corporations. The exploitation of workers and the race to the bottom creates dishonesty and a lack of transparency. While there are many points that I agree with, I found Gebru’s talk to be lacking in a lot of ways. The connection between Eugenics and AGI felt like a large leap. There are many goals and ideas that are shared between the communities, but I do not understand the connection between the two. I also found her talk to be lacking in concrete evidence. That also made me skeptical about her lecture. I would also like to discuss her reaction to challenges and disagreements in her argument. The question-and-answer portion of her lecture seemed unprofessional. When presented with a question that opposed her opinions, she immediately shut the audience member down, exclaiming that it was inappropriate to ask such questions. That discredited her even more because having a strong understanding in a topic suggests that you understand both sides of an argument. Having a professional conversation about a controversial topic is much more beneficial and persuasive if the conversation presents facts and is calm. The fact that she refused to hear and answer someone’s question that challenges her beliefs partially suggests that she lacks the understanding of the other side.\nI thoroughly enjoyed hearing about her background in our in-class portion. She paved the way for many POC women in computer science. And her experiences are inspiring. However, I was set back by the lecture aspect. I understand that she’s extremely passionate about eugenics and AIG. But after that lecture, I am skeptical about her research."
  },
  {
    "objectID": "posts/PenguinPost/Penguin.html",
    "href": "posts/PenguinPost/Penguin.html",
    "title": "Hello Blog",
    "section": "",
    "text": "Part 1: Exploring the data set\nWhen exploring the data set, I wanted to first explore something simple, so I started with an exploration of sex and body mass among each species. And what I found was that in all three species, the male sex has a higher body mass on average than the female sex. Furthermore, we can see that the gentoo penguin has a higher body mass than the adelie penguin and the chinstrap penguin. The table with such information is displayed below as well.\nWhen I first created the scatterplot, the dots overlapped each other, making it difficult to read. So, I created a box plot underneath the scatterplot as well. When I do that, I notice that the Chinstrap male species is lighter than the Adelie male species on average, but the Chinstrap female species if heavier than the Adelie female species on average.\nFurthermore, I wanted to explore the relationship between flipper length and body mass in a second graph. I created a scatterplot, ignoring species. I found a positive correlation between body mass and flipper length. As body mass increases, flipper length does as well, and vice versa. The table further emphasizes that as well. Breaking it down by species, we can see that the Gentoo pengiuns, who have the highest body mass on average, subsequently have the longer flipper length on average. And the Adelie penguins, who have the lowest body mass on average, have the shortest flipper length on average.\nPart 2: Finding three features of the data and a model trained on those features which achieves 100% testing accuracy\nFirst, I started by using the starter code provided to us to find the three features which achieves a 100% testing accuracy. Furthermore, I decided that I wanted to use a Random Forest Classifier to train my model. With that, I was able to start writing my code.\nWith my code, we have three lists, all_qual_cols which contains the names of the categorical columns, all_quant_cols which contains the names of the numerical columns, and best_cols which will store the best combination of features. Within the loop, we will iterate over each categorical column and all possible combinations of two numerical columns. We will get those three columns and train a Random Forest Classifier, imported from sklearn, with 100 trees and a random state of 0. We will then generate predictions for the training data using the trained model and evaluate the performance using the accuracy_score function. If the accuracy of the model on the training data is higher than the previous best accuracy, the best accuracy and the best combination of features will be updated. At the end of the loop, we will print out the best accuracy and the best set of features. We see that Culmen Length, Culmen Completion, and Culmen Depth are the three best features.\nKnowing these features, we will use them with and train a random forest classifier on our testing data. Afterwards, we find that we get an accuracy rate of 97%, which is relatively great!\nPart 3: Plotting Decision Regions\nWe then plot the decision regions. I use the code given to us, and I find that the decision regions look very good!\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nsns.scatterplot(x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\", data = train)\nplt.show()\n\n\n\n\n\nsns.boxplot(x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\", data = train)\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\nraw_data = pd.read_csv(train_url)\nX_train, y_train = prepare_data(raw_data)\n\n\ntrain = pd.get_dummies(train, columns = ['Sex'], drop_first = \"if_binary\")\n\ntrain.groupby('Species')[['Body Mass (g)', 'Sex_MALE']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Sex_MALE\n    \n    \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.09\n      118\n      0.47\n      118\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.86\n      56\n      0.48\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5119.50\n      101\n      0.53\n      101\n    \n  \n\n\n\n\n\nWhen looking at the species of penguins and their body mass, I can see that the Gentoo penguins are, on average, much larger in body mass than the chinstrap penguins and the adelie penguins.\n\nsns.scatterplot(data=train, x='Body Mass (g)', y='Flipper Length (mm)')\n\n<AxesSubplot:xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\n\ngrouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\ngrouped\n\n/var/folders/k5/sv5thph578dff6sqh3h3z2kw0000gn/T/ipykernel_43467/3046570246.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  grouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\n\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      median\n      std\n      mean\n      median\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.094017\n      3650.0\n      455.209898\n      189.965812\n      190.0\n      6.678493\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.857143\n      3687.5\n      404.876925\n      195.464286\n      195.0\n      7.032300\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5119.500000\n      5100.0\n      509.661454\n      217.650000\n      217.0\n      6.406081\n    \n  \n\n\n\n\nDescription of the scatterplot and the table above:\nLooking at the scatter plot, we see that there is a positive correlation between flipper length and body mass. As flipper length increases, the penguin’s body mass does as well.\nLooking at the table, we can see that positive correlation between flipper length and body mass. We can also see that the Adelie Penguin has the smallest body mass and the smallest flipper length among the group. Gentoo Penguins, on the other hand, have the largest body mass and the largest flipper length among the group.\n\nfrom itertools import combinations\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nbest_cols = []\nbest_acc = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n        X_train_sub = X_train[cols]\n        clf = RandomForestClassifier(n_estimators=100, random_state=0)\n        clf.fit(X_train_sub, y_train)\n        y_pred = clf.predict(X_train_sub)\n        acc = accuracy_score(y_pred, y_train)\n        if(acc > best_acc):\n            best_acc = acc\n            best_cols = cols\n\nprint(\"best accuracy:\", best_acc)\nprint(\"best columns:\", best_cols)\n\nbest accuracy: 1.0\nbest columns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_train_sub = X_train[best_cols]\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train_sub, y_train)\n\nX_test, y_test = prepare_data(test)\ny_pred = clf.predict(X_test[best_cols])\nacc = accuracy_score(y_pred, y_test)\n\nprint(\"accuracy:\", acc)\n\naccuracy: 0.9705882352941176\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\n\nplot_regions(clf, X_test[best_cols], y_test)"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\n\nLR = LogisticRegression()\nLR.fit(X,y)\nplot_decision_regions(X,y,clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X,y)}\")\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\nLR2 = LogisticRegression()\n\nX_ = X**2\nLR2 = LogisticRegression();\nLR2.fit(X_, y)\nplot_decision_regions(X_, y, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_, y)}\")\n#\n#\n#\n#"
  },
  {
    "objectID": "Gradient.html",
    "href": "Gradient.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from solutions.logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nModuleNotFoundError: No module named 'solutions'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Limits of the Quantitative Approach\n\n\n\n\n\n\nMay 15, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDescription of everything we accomplished for our project\n\n\n\n\n\n\nMay 10, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing Linear Regression\n\n\n\n\n\n\nMay 2, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy Classifying Pengiuns Blog Post.\n\n\n\n\n\n\nApr 25, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach Essay\n\n\n\n\n\n\nApr 18, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Perceptron.html",
    "href": "Perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "p = perceptron.Perceptron()\n\np.fit(X, y, 1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2"
  },
  {
    "objectID": "posts/FinalProject/FinalProject.html",
    "href": "posts/FinalProject/FinalProject.html",
    "title": "Human Activity Recognition (HAR) - Final Project",
    "section": "",
    "text": "Human Activity Recognition has become widely used and valued. But what is human activity recognition? The article entitled, “Human Activity Recognition in Artificial Intelligence Framework: A Narrative Review”, would define Human Activity Recognition (HAR) as “the art of identifying and naming activities using Artificial Intelligence (AI) from the gathered activity raw data by utilizing various sources (so-called devices)” (Guptna, N., Gupta, S.K., Pathal, R.K. et al.). In other words, HAR is a machine learning concept that determines what humans are doing any given point in time. This includes walking, running, sitting, standing, walking upstairs, biking, etc. In fact, HAR is used constantly. Iphones tell people how much they walk in a day, apple watches track the amount of physical activity people get, and HAR has found its way in healthcare, surveillance, and remote care to the elderly.\nLiterature Review:\nDiverging from the usage of HAR, a big and current topic is how to implement HAR. Since 2006, researchers and computer scientists have implemented different algorithms to determine the best algorithm to use. In the article titled, High Accuracy Human Activity Recognition Using Machine Learning and Wearable Devices’ Raw Signals, the authors explained the history of HAR research and the different algorithms used. First in 2006, Pirttijangas et al. “tested a model that used several multilayer perceptron and k-nearest neighbor’s algorithms to recognize 17 activities to achieve an overall accuracy and 90.61%” (Papaleonidas, Psathas, and Iliadis). In 2011, Casale et al. “used a wearable device and applied a random forest classification algorithm to model five distinct activities (walking, climbing stairs, talking to a person, standing, and working on the computer)”, which achieved a 90% accuracy (Papaleonidas, Psathas, and Iliadis). In 2018, Brophy et al. “proposed a hybrid convolutional neural network and an SVM model with an accuracy of 92.3% for four activities (walking and running on a treadmill, low and high resistance bike exercise)” (Papaleonidas, Psathas, and Iliadis).\nThere are many ways and many algorithms that people have attempted to implement to find the best and most accurate model for Human Activity Recognition.\nLooking at these different implementations of Human Activity Recognition, we wanted to implement three algorithms, the k-nearest neighbor algorithm, the multilayer perceptron (also known as a fully connected neural network), and a random forest classifier, to determine the best accuracy given our data set. More about our data set can be found below.\n\nfrom os import listdir\nfrom pandas import read_csv\nimport pandas as pd\nimport numpy as np\nimport glob\nimport random\nfrom pandas import DataFrame\nfrom matplotlib import pyplot\nfrom numpy import vstack\nfrom numpy import unique\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nFirst, let’s load in our data set using the three functions below:\n\n# load a single file as a numpy array\ndef load_file(filepath):\n    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n    return dataframe.values\n\n# load a group of files, such as x, y, z data for a given variable and  return them as a 3d numpy array\ndef load_group(filenames, prefix=''):\n    loaded = list()\n    for name in filenames:\n        data = load_file(prefix + name)\n        loaded.append(data)\n    # stack group so that features are the 3rd dimension\n    loaded = np.dstack(loaded)\n    return loaded\n\n# load a dataset type, such as train or test\ndef load_dataset(type, prefix=''):\n    path = prefix + type + '/Inertial Signals/'\n    \n    filenames = list()\n    # total acceleration\n    filenames += ['total_acc_x_' + type + '.txt', 'total_acc_y_' + type + '.txt', 'total_acc_z_' + type + '.txt']\n    # body acceleration\n    filenames += ['body_acc_x_' + type + '.txt', 'body_acc_y_' + type + '.txt', 'body_acc_z_' + type + '.txt']\n    # body gyroscope\n    filenames += ['body_gyro_x_' + type + '.txt', 'body_gyro_y_' + type + '.txt', 'body_gyro_z_' + type + '.txt']\n\n    # load input data\n    X = load_group(filenames, path)\n    # load output data\n    y = load_file(prefix + type + '/y_'+type+'.txt')\n    return X, y\n\nAs we load in the dataset, here is a description of the dataset as found in the ReadMe of the dataset:\n“The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix ‘t’ to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the ‘f’ to indicate frequency domain signals). These signals were used to estimate variables of the feature vector for each pattern: ‘-XYZ’ is used to denote 3-axial signals in the X, Y and Z directions” (Reyes-Ortiz et al.)\nNow let’s take a closer look at our data:\n\n# summarize the balance of classes in an output variable column\ndef class_breakdown(data):\n    # convert the numpy array into a dataframe\n    df = DataFrame(data)\n    # group data by the class value and calculate the number of rows\n    counts = df.groupby(0).size()\n    # retrieve raw rows\n    counts = counts.values\n    # summarize\n    for i in range(len(counts)):\n        percent = counts[i] / len(df) * 100\n        print('Class=%d, total=%d, percentage=%.3f' % (i+1, counts[i], percent))\n\n\n# load all train data\nprint('Training data')\ntrainX, trainy = load_dataset('train', 'UCI HAR Dataset/')\nclass_breakdown(trainy)\n# load all test data\nprint('Testing data')\ntestX, testy = load_dataset('test', 'UCI HAR Dataset/')\nclass_breakdown(testy)\n\nprint('Both')\ncombined = vstack((trainy, testy))\nclass_breakdown(combined)\n\nTraining data\nClass=1, total=1226, percentage=16.676\nClass=2, total=1073, percentage=14.595\nClass=3, total=986, percentage=13.411\nClass=4, total=1286, percentage=17.492\nClass=5, total=1374, percentage=18.689\nClass=6, total=1407, percentage=19.138\nTesting data\nClass=1, total=496, percentage=16.831\nClass=2, total=471, percentage=15.982\nClass=3, total=420, percentage=14.252\nClass=4, total=491, percentage=16.661\nClass=5, total=532, percentage=18.052\nClass=6, total=537, percentage=18.222\nBoth\nClass=1, total=1722, percentage=16.720\nClass=2, total=1544, percentage=14.992\nClass=3, total=1406, percentage=13.652\nClass=4, total=1777, percentage=17.254\nClass=5, total=1906, percentage=18.507\nClass=6, total=1944, percentage=18.876\n\n\nWe can see above that our data is equally split among the six activities. This is a good sign and tells us there is an equal distribution among the activities.\n\n# looking at the training data - which subjects are used?\nsub_map = load_file('UCI HAR Dataset/train/subject_train.txt')\ntrain_subjects = unique(sub_map)\nprint(train_subjects)\n\n[ 1  3  5  6  7  8 11 14 15 16 17 19 21 22 23 25 26 27 28 29 30]\n\n\n\n# get all data for one subject\ndef data_for_subject(X, y, sub_map, sub_id):\n    # get row indexes for the subject id\n    ix = [i for i in range(len(sub_map)) if sub_map[i]==sub_id]\n    # return the selected samples\n    return X[ix, :, :], y[ix]\n\n\n# convert a series of windows to a 1D list\ndef to_series(windows):\n    series = list()\n    for window in windows:\n        # remove the overlap from the window\n        half = int(len(window) / 2) - 1\n        for value in window[-half:]:\n            series.append(value)\n    return series\n\nNow that we have fully loaded in the data and updated the data, we should take a look at what the data looks like. Below, we will plot two distinct graphs. The first graph will consist of two similar graphs from two different subjects. It will show the total acceleration, the body acceleration, and the body gyroscope for each activity over time.\n\ndef plot_subject(X, y, person):\n    pyplot.figure()\n    n, off = X.shape[2] + 1, 0\n    pyplot.suptitle('Person ' + str(person), y = 0.93)\n\n    # total acceleration\n    for i in range(3):\n        pyplot.subplot(n, 1, off + 1)\n        pyplot.plot(to_series(X[:, :, off]))\n        pyplot.title('total acc ' + str(i), y = 0, loc = 'left')\n        off += 1\n    \n    # body acceleration\n    for i in range(3):\n        pyplot.subplot(n, 1, off + 1)\n        pyplot.plot(to_series(X[:, :, off]))\n        pyplot.title('body acc ' + str(i), y = 0, loc = 'left')\n        off += 1\n    \n    # body gyro\n    for i in range(3):\n        pyplot.subplot(n, 1, off + 1)\n        pyplot.plot(to_series(X[:, :, off]))\n        pyplot.title('body gyro ' + str(i), y = 0, loc = 'left')\n        off += 1\n    \n    pyplot.subplot(n, 1, n)\n    pyplot.plot(y)\n    pyplot.title('activity', y=0, loc='left')\n    pyplot.show()\n\n\n# # load data\n# trainX, trainy = load_dataset('train', 'UCI HAR Dataset/')\n# # load mapping of rows to subjects\n# sub_map = load_file('UCI HAR Dataset/train/subject_train.txt')\n# train_subjects = unique(sub_map)\n# print(train_subjects)\n# get the data for one subject\nperson = random.randint(0, len(train_subjects))\nsub_id = train_subjects[person]\nsubX, suby = data_for_subject(trainX, trainy, sub_map, sub_id)\nprint(subX.shape, suby.shape)\n# plot data for subject\nplot_subject(subX, suby, person)\n\n(316, 128, 9) (316, 1)\n\n\n\n\n\n\n# get the data for one subject\nperson2 = random.randint(0, len(train_subjects))\nif person2 == person:\n    person2 = random.randint(0, len(train_subjects))\nsub_id = train_subjects[person2]\nsubX, suby = data_for_subject(trainX, trainy, sub_map, sub_id)\nprint(subX.shape, suby.shape)\n# plot data for subject\nplot_subject(subX, suby, person2)\n\n(409, 128, 9) (409, 1)\n\n\n\n\n\nWithin the plots above we see different levels of activity. For the walking activities (walking, walking upstairs, and walking downstairs), which are associated with activities 1, 2, and 3, we see a greater amount of movement compared to activities 4, 5, and 6, which correspond to sitting, standing, and laying. When we see “stagnant” lines, we can correspond that to activities 4, 5, and 6.\nIn addition, for both people, the accelerations look very similar as they are completing the same tasks.\n\n# group data by activity\ndef data_by_activity(X, y, activities):\n    # create a dictionary where the activity is the key, and the subset of X is the value\n    result = {}\n    for a in activities:\n        mask = y[:, 0] == a\n        subset_X = X[mask, :, :]\n        result[a] = subset_X\n    return result\n\n\ndef plot_activity_histograms(X, y):\n    # get a list of unique activities for the subject\n    activity_ids = unique(y[:,0])\n    grouped = data_by_activity(X, y, activity_ids)\n    \n    # plot histograms per activity\n    pyplot.figure()\n    pyplot.suptitle('Person ' + str(person) + \"'s Total Acceleration for the Different Activities\", y = 0.93)\n    xaxis = None\n    for k in range(len(activity_ids)):\n        activ_id = activity_ids[k]\n\n        for i in range(3):\n            ax = pyplot.subplot(len(activity_ids), 1, k+1, sharex=xaxis)\n            ax.set_xlim(-1,1)\n            if k == 0:\n                xaxis = ax\n            # create histogram subplot by indexing the grouped activities dictionary by activity and total acceleration axis\n            pyplot.hist(to_series(grouped[activ_id][:,:,i]), bins=100)\n            pyplot.title('activity '+str(activ_id), y=0, loc='left')\n    pyplot.show()\n\n\nplot_activity_histograms(subX, suby)\n\n\n\n\nTaking a look at the histogram above, we can see the total acceleration for each activity. We can see how some activities look vastly different from each other. For instance, activity 6 (laying down) is extremely different than activity 1 (walking).\nIt is also important to not the three colors, which correspond to the three axis. The x-axis is blue, the y-axis is orange, and the z-axis is green.\nNow that we have an understanding of what our data looks like, it is time to run some algorithms. We ran three algorithms on our data. The first algorithm used was the K-nearest neighbors algorithm, imported from sklearn. The second algorithm is a fully connected neural network, imported from sklearn. And the third and last algorithm is a random forest classifier, also imported from sklearn.\nThe three algorithms are seen below:\n\n# k Nearest Neighbors algorithm (imported from sklearn)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(trainX, trainy, test_size=0.33, random_state=42)\n\n# k-nearest neighbors classifier with k=1\nknn = KNeighborsClassifier(n_neighbors=1)\n\n# fit the model on training data\nknn.fit(X_train.reshape(X_train.shape[0], -1), y_train.ravel())\n\n# predict the test data\ny_pred = knn.predict(X_test.reshape(X_test.shape[0], -1))\n\n# calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"kNN Accuracy:\", accuracy)\n\nkNN Accuracy: 0.8978162340337865\n\n\n/Users/kaylynnxia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n\n\nExplanation of the KNN algorithm:\nWe implemented KNN on our data set. We started by splitting the data into train and test data. Then we trained the KNN on the training data (we had to reshape the training data) with 1 neighbors. We tested out a few different values for n_neighbors, and we found that k = 1 gives us the best results. Finally, we predicted on testing data and got the accuracy score.\n\n# fully connected neural network (multilayer perceptron)\n\n# reshape data\nn_features = trainX.shape[1] * trainX.shape[2]\ntrainX = trainX.reshape((trainX.shape[0], n_features))\ntestX = testX.reshape((testX.shape[0], n_features))\n\n# define the model\nmodel = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500)\n\n# fit the model on the training data\nmodel.fit(trainX, trainy.ravel())\n\n# calculate the accuracy\naccuracy = model.score(testX, testy)\nprint('Fully Connected Neural Network Accuracy:', accuracy)\n\nFully Connected Neural Network Accuracy: 0.8554462164913471\n\n\nExplanation of the fully connected neural network:\nWe implemened this from sklearn. We first loaded in the dataset and then reshaped the train and test data so that if works with the model. We fit the data and found the accuracy score.\n\n# Random Forest Classifier\n\n# create a random forest classifier with 100 trees\nrfc = RandomForestClassifier(n_estimators = 100)\n\n# train on the training data\nrfc.fit(trainX.reshape(len(trainX), -1), trainy.ravel())\n\n# predict the test data\ny_pred = rfc.predict(testX.reshape(len(testX), -1))\n\n# calculate the accuracy\naccuracy = accuracy_score(testy, y_pred)\nprint(\"Random Forest Accuracy:\", accuracy)\n\nRandom Forest Accuracy: 0.8534102477095351\n\n\nExplanation of the random forest classifier:\nSimilar to what we did previously, we implemented the random forest classifier using 100 trees. We had to reshape the training data before trainign the data using the algorithm. After, we predicted the test data and got the accuracy score.\n\ntrainX, trainy = load_dataset('train', 'UCI HAR Dataset/')\nX_train, X_test, y_train, y_test = train_test_split(trainX, trainy, test_size=0.33, random_state=42)\n\n#Average accuracy of knn for 10 models\n\n# k-nearest neighbors classifier with k=1\nknn = KNeighborsClassifier(n_neighbors=1)\n# fit the model on training data\nknn.fit(X_train.reshape(X_train.shape[0], -1), y_train.ravel())\n# predict the test data\ny_pred = knn.predict(X_test.reshape(X_test.shape[0], -1))\n# calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"kNN Accuracy:\", accuracy)\n\n#Average accuracy of neural networks for 10 models\n\nmlp_models = []\n\nfor i in range(10):\n    n_features = trainX.shape[1] * trainX.shape[2]\n    trainX_mlp = trainX.reshape((trainX.shape[0], n_features))\n    testX_mlp = testX.reshape((testX.shape[0], n_features))\n\n    model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500)\n    model.fit(trainX_mlp, trainy.ravel())\n    mlp_models.append(model.score(testX_mlp, testy))\nprint(\"Average accuracy for neural networks \" + str(np.mean(mlp_models)))\n\n\n#Average accuracy of random forest classifier for 10 models\n\nrfc_models = []\n\nfor i in range(10):\n    rfc = RandomForestClassifier(n_estimators = 100)\n    rfc.fit(trainX.reshape(len(trainX), -1), trainy.ravel())\n    y_pred = rfc.predict(testX.reshape(len(testX), -1))\n    rfc_models.append(accuracy_score(testy, y_pred))\nprint(\"Average accuracy for random forest classifier: \"+ str(np.mean(rfc_models)))\n\n/Users/kaylynnxia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n\n\nkNN Accuracy: 0.8978162340337865\nAverage accuracy for neural networks 0.8529691211401426\nAverage accuracy for random forest classifier: 0.846929080420767\n\n\nFrom the three algorithms above, we can see that the accuracy scores are similar in all three algorithms. However, the K Nearest Neighbors Algorithm produced an accuracy score of 89.8%, the fully connected neural network produced an accuracy score of 85.3%, and the Random Forest Classifier produced an accuracy score of 84.7%.\nGiven this, we can conclude that the K Nearest Neighbors Algorithm produced the best results.\nIt is important the note the biases associated with our data and model.\nMost would agree that the broad usage fo HAR is used to benefit people’s physical health. Most usage includes tracking exercise. However, sometimes these models can also be used in a negative way. In the article titled, Vision-based Human Activity Recognition: A Survey, the researchers found that HAR has been used for surveillance and security. When facial recognition isn’t present, human recognition can be used to target people based on things like posture and walking pattern (Beddiar, D.R., Nini, B., Sabokrou, M. et al.). This is obviously very controversial. As we have seen previous with COMPAS, these security measures often target innocent people and are racially biased.\nFurthermore, we found issues associated with our data. Our data only includes people between the ages of 19-48. This can be problematic because it excludes children, many teens, and the elderly, which could make our results skewed. It is also important to note that the data only contains 30 volunteers, which is a very small number and isn’t representative of the general population. While not necessarily problematic, it makes our results inaccurate.\nIf we had more time, we could vastly improve our project.\nFirst, we would first run more our data with more algorithms, including convolutional neural networks and a support vector machine.\nFurthermore, I would use other HAR data and run the same algorithms on that data.\nFinally, it would be nice to split the data into window data as opposed to snapshot data and running the algorithms that way.\nSources:\nGupta, N., Gupta, S.K., Pathak, R.K. et al. Human activity recognition in artificial intelligence framework: a narrative review. Artif Intell Rev 55, 4755–4808 (2022). https://doi.org/10.1007/s10462-021-10116-x\nAntonios Papaleonidas, Anastasios Panagiotis Psathas & Lazaros Iliadis (2022) High accuracy human activity recognition using machine learning and wearable devices’ raw signals, Journal of Information and Telecommunication, 6:3, 237-253, DOI: 10.1080/24751839.2021.1987706\nBeddiar, D.R., Nini, B., Sabokrou, M. et al. Vision-based human activity recognition: a survey. Multimed Tools Appl 79, 30509–30555 (2020). https://doi.org/10.1007/s11042-020-09004-3\nReyes-Ortiz, Jorge L, et al. “Human Activity Recognition Using Smartphones Dataset.” UCI Machine Learning Repository, Nov. 2013, archive.ics.uci.edu/ml/index.php."
  },
  {
    "objectID": "posts/FinalProject/FinalBlogPost.html",
    "href": "posts/FinalProject/FinalBlogPost.html",
    "title": "Hello Blog - Project Blog Post",
    "section": "",
    "text": "Human Activity Recognition (HAR) is the act of identifying and naming activities given some raw data through devices. You can think of a smart watch tracking how many steps you take in a day. You can think of surveilling people based on their walking patterns. HAR is a relatively new concept and has not yet been perfected. Different researchers have used different algorithms to determine what the best algorithm is. We wanted to take three algorithms – K-Nearest Neighbors, Multilayer Perceptron, which is also known as a Fully Connected Neural Network, and Random Forest Classifier – and test it on our HAR data to see which one is actually the best. We found that a K-Nearest Neighbors algorithm produced the best results, achieving an accuracy score of nearly 90%. The neural network and the random forest classifier produced similar results, achieving an accuracy score of 85% and 84%.\nLink to our GitHub repository: https://github.com/kaylynnx/MLProject/blob/main/Code.ipynb\n\nIntroduction\n\nIn 2006, Pirrttikangas et al. pioneered the research on Human Activity Recognition. In their research, they “tested a model that used several multilayer perceptron and k-nearest neighbor’s algorithms to recognize 17 activities to achieve an overall accuracy of 90.61%” (Papaleonidas, Psathas, and Iliadis (2021)). Since then, there have been a multitude of papers laying out different algorithms used to train HAR data. Some algorithms achieved significantly better results than others. To name just a few, Casale et al. in 2011 “applied a random forest classification algorithm to model five distinct activities (walking, climbing stair, talking to a person, standing, and working on the computer)” (Papaleonidas, Psathas, and Iliadis (2021)). In 2013, Ahmen and Loutfi used “case-based reasoning, support vector machines (SVMs) and neural networks (NN) to achieve an overall accuracy of 0.86, 0.62, and 0.59 respectively” (Papaleonidas, Psathas, and Iliadis (2021)). And in 2018, Brophy et al. “proposed a hybrid convolutional neural network and an SVM model with an accuracy of 92.3% for four activities (walking and running on a treadmill, low and high resistance bike exercise)” (Papaleonidas, Psathas, and Iliadis (2021)).\nCircling back to the 2006 study, another research paper published in 2021, Human Activity Recognition Using K-Nearest Neighbor Machine Learning Algorithm, presented a k-nearest neighbor algorithm for classification of human activities, namely laying, walking downstairs, sitting, walking upstairs, standing, and walking. They found that the results prove, “a high performance in the classification of human activities” (Mohsen, Elkaseer, and Scholz (2021)). They evaluated the accuracy score using a few different metrics and found the “weighted average precision, F1-score, and the area under the micro-average precision-recall curve for the KNN are 90.96%, 90.46%, 90.37%, and 96.5%, respectively, while the area under the ROC curve is 100%” (Mohsen, Elkaseer, and Scholz (2021)). As a result, these authors claim that a lot of literature has attempted to classify human activities, but the k-nearest neighbors algorithm has “shown the highest potential to address the accuracy issue in HAR” (Mohsen, Elkaseer, and Scholz (2021)).\n\nValue Statement\n\nIn most cases, HAR is used for personal care. This includes care to the elderly, medicine, and self-improvement, which includes exercise. When it comes to medicine and care, HAR is certainly beneficial. However, with every great idea, there are usages that are a cause for concern. With that in mind, there are a lot of people – young, old, healthy, and unhealthy – that would benefit from HAR. Medicine can improve, people can track their fitness, and everyone can use it as a good source of data. (Gupta et al. (2022))\nHowever, recently, HAR has been used for surveillance and security. In the article titled, Vision-based Human Activity Recognition: A Survey, the researchers found that HAR has been used for surveillance and security. When facial recognition isn’t present, human recognition can be used to target people based on things like posture and walking pattern (Beddiar et al. (2020)). This is obviously very controversial. Such a tool can be used to target minorities and people of color. It could have target those who are innocent. In this sense, many underrepresented people would suffer.\n\nMaterials and Methods\n\nWe used the dataset from the UCI Machine Learning Repository. This dataset includes 30 volunteers between the ages of 19 and 48 who wore a Samsung Galaxy phone on their waist and were requested to perform a series of six activities. These activities are standing, sitting, laying, walking, walking downstairs, and walking upstairs. The phone calculated movement in the x-axis, y-axis, and z-axis. The creators of the dataset were able to extract just total acceleration, body acceleration, and body gyroscope in the x-axis, y-axis, and z-axis. Furthermore, the dataset was pre-split into the training data and the testing data. 70% of the dataset (21 subjects) were used for the training data, and the rest (9 subjects) were used for the testing data. The data set can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip (Reyes-Ortiz et al. (2013)).\nIt is also important to note the limitations from the UCI Machine Learning Dataset. The dataset only includes 30 volunteers. Regardless of who these volunteers are, it is not enough people to be a good representation of the overall public. The small number of volunteers is also cause for bias. This leads us to the fact that the volunteers are between the ages 19-48. This is a young and skillful population, which could cause some inaccuracies, especially if we use our code to classify young children or the elderly.\nFor our method, we wanted to take the article about the K-nearest neighbors’ algorithm being the best algorithm for classification and either prove or disprove it. So, we knew that we wanted to train our data using the KNN algorithm. Furthermore, both the multilayer perceptron (also known as a fully connected neural network) and the random forest classifier algorithms resulted in a very good accuracy score. To test out the theory that KNN is the best algorithm, we ran this algorithm against the two other algorithms. To evaluate performance, we looked at the accuracy score, as calculated by sklearn.\n\nResults\n\nWe outputted 2 distinct graphs first. When we look at the first chart, we can see total acceleration, body acceleration, and body gyroscope in the x-, y-, and z-axes for a single subject. The first subject just happens to be subject 9 and the second is subject 19. These are random. When you look at this chart, we can see that there is a lot more movement around activities 1, 2, and 3, which are the walking activities. This is walking, walking upstairs, and walking downstairs. Similarly, there is a lot less movement and acceleration during activities 4, 5, and 6 because those are the static movements (sitting, standing, laying). We can also see that for the two-subject displayed, the patterns are very similar. The subjects do the same activities at the same time, and the corresponding acceleration and gyroscope movements are roughly the same.\nWe can also take a look at only total acceleration in the three axes for all activities. The graph displayed happens to show subject 9. In this graph, the blue color is associated with the x-axis, the orange is associated with the y-axis, and green is associated with the z-axis. We can see those certain activities, such as activities 2 and 3 (walking upstairs and walking downstairs) have similar movements in the x-, y-, and z- direction which may make it difficult to classify. Regardless, each activity has it nuances that we can train.\nGiven that, we can train our data with the k-nearest neighbor, the multilayer perceptron, and the random forest classifier algorithms. When we do so, we find that k-nearest-neighbors with n = 1, gives us an accuracy score of 89.9%, multilayer perceptron gives us an accuracy score of 85.7%, and random forest classifier with 100 trees gives us an accuracy score of 84.7%. With that in mind, I can conclude that the K-Nearest Neighbors algorithm, with the number of neighbors equal to 1, was the best algorithm to train our HAR dataset.\nHowever, it is important to understand that further experimentation is needed to definitively say that KNN is better than every other algorithm. All algorithms produce similar scores.\n\nConcluding Discussion\n\nThrough our experimentation, we found that KNN was better than multilayer perceptron and random forest classifier. However, more experimentation is needed. If we had more time, we could look at window data instead of snapshot data. We could also look at some other algorithms such as a support vector machine or other neural networks, such as a convolutional NN. It would also be important to look at other, more inclusive data sets and train the same algorithms on them as well. With that in mind, our project supports the research paper, Human Activity Recognition Using K-Nearest Neighbor Machine Learning Algorithm by Saeed Mohsen, Ahmed Elkaseer, and Steffen G. Scholz.\nOur project definitely worked well. We were able to gather data and clean it up a little bit, though most of the tedious work was done for us. We were able to look at some graphs and figure out what question we wanted to answer. Our goal was to use a few predictive models on our data, and we were able to do that. We have well-documented and clean code as well. I would say that we were very successful!\n\nGroup Contribution\n\nI started by doing the literature review. I read a couple of papers and came up with experimentation. Zayn found the dataset that we worked with. Then, Zayn and I worked on loading in the dataset, which corresponds to the first three functions. I wrote the class_breakdown, data_for_subject, series, and plot_subject functions. I was able to output a plot, and Zayn helped edit the outputs, so they looked a little bit prettier. Zayn wrote the code for the second plot, which is a histogram of total acceleration in the x-axis, y-axis, and z-axis. I also wrote the code for the algorithms, and Mead helped put the code in a for loop. Finally, I wrote all of the embedded writing, describing our outputs and decision making.\n\nPersonal Reflection\n\nThrough this project, I learned a lot about how people experiment, test, and find the most optimal algorithm or algorithms. People extensively test and experiment and then ultimately, come up with a conclusion. It feels very scientific, which is an experience I haven’t experienced in previous computer science classes.\nI also feel that while my project could be a lot more in depth, I exceeded my own expectations with this project. I was able to be a great teammate and leader. I assigned different roles to different people. I also did a lot of work on this project. I researched about biases and social responsibility, learned a lot more about experimentation, and I practiced my teamwork and collaboration skills.\nI will take a lot away with me. I learned a lot more about how experimentation works and how people come up with experiments. Most of the time, people attempt to prove or disprove a research paper. Other times people follow and recreate published research. I also will take away how to better work in a group. I learned how to organize better and how to better manage people.\nSources:\nGupta et al. (2022)\nPapaleonidas, Psathas, and Iliadis (2021)\nBeddiar et al. (2020)\nReyes-Ortiz et al. (2013)\nMohsen, Elkaseer, and Scholz (2021)\n\n\n\n\nReferences\n\nBeddiar, Djamila Romaissa, Brahim Nini, Mohammad Sabokrou, and Abdenour Hadid. 2020. “Vision-Based Human Activity Recognition: A Survey.” Multimedia Tools and Applications 79 (41–42): 30509–55. https://doi.org/10.1007/s11042-020-09004-3.\n\n\nGupta, Neha, Suneet K. Gupta, Rajesh K. Pathak, Vanita Jain, Parisa Rashidi, and Jasjit S. Suri. 2022. “Human Activity Recognition in Artificial Intelligence Framework: A Narrative Review.” Artificial Intelligence Review 55 (6): 4755–4808. https://doi.org/10.1007/s10462-021-10116-x.\n\n\nMohsen, Saeed, Ahmed Elkaseer, and Steffen G. Scholz. 2021. “Human Activity Recognition Using k-Nearest Neighbor Machine Learning Algorithm.” SpringerLink. Springer Singapore. https://link.springer.com/chapter/10.1007/978-981-16-6128-0_29.\n\n\nPapaleonidas, Antonios, Anastasios Panagiotis Psathas, and Lazaros Iliadis. 2021. “High Accuracy Human Activity Recognition Using Machine Learning and Wearable Devices’ Raw Signals.” Journal of Information and Telecommunication 6 (3): 237–53. https://doi.org/10.1080/24751839.2021.1987706.\n\n\nReyes-Ortiz, Jorge L, Alessandro Ghio, Luca Oneto, Davide Anguita, and Xavier Parra. 2013. “Human Activity Recognition Using Smartphones Data Set.” UCI Machine Learning Repository: Human Activity Recognition Using Smartphones Data Set. https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones."
  }
]