[
  {
    "objectID": "posts/LimitsQuantitativeApproach/Essay.html",
    "href": "posts/LimitsQuantitativeApproach/Essay.html",
    "title": "Hello Blog",
    "section": "",
    "text": "As long as humans have existed, technology has been a powerful force. From the wheel to running water to the computer, technology has aided our day-to-day lives. In modern-day technology, since the mid-1900s, technology has expanded past new inventions to more quantitative innovations. The generation of new phones and computers has become relatively stagnant, but the usage of phones and computers to aid our day-to-day lives has exploded. Scientists and businessmen created Machine Learning, more colloquially known as Artificial Intelligence. Artificial Intelligence has been displayed in movies such as Robocop or the recently released horror movie, Megan. These movies depict a universe in which Artificial Intelligence is self-aware and relatively human. While largely exaggerated, we have created machines to learn from data, hence why it is called Machine Learning. Given some historical data or a collection of current data, the machines can generate some sort of conclusion.\nMachine Learning is used in the finance industry to uncover credit card fraud, make predictions about creditworthiness, and identify trends in the stock market (Ethical Implications of Bias in Machine Learning). Machine learning has been incredibly productive and helpful to provide important advances in health care and treatment decisions (Ethical Implications of Bias in Machine Learning). The criminal justice system is using machine learning to predict crime hotspots and recidivism rates (Ethical Implications of Bias in Machine Learning). AI and Machine learning has been increasingly widespread. They have been helpful in all aspects of our lives. But with all great innovations, there are unintended consequences. One of the unintended consequences is bias. What is bias? When we think of bias or fairness in a social, political, or economic sense, bias is defined as prejudice against one person, group, or thing in a way classified as ‘unfair’. Essentially, bias in an algorithmic sense, which we coin algorithmic bias, is very similar. Most researchers in this field define algorithmic bias differently. Nevertheless, “most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status” (Bias in Machine Learning – What is it Good For). What does this mean exactly? Well, bias in Machine Learning isn’t so black or white. Instead, the authors of Bias in Machine Learning found that biases can be organized into three distinct categories: “a biased world, data generation, and learning” (Bias in Machine Learning – What is it Good For). Within a biased world, bias may propagate through what people would refer to as historical bias. According to the Australian Human Rights Commission, “historical bias arises when the data used to train an AI system no longer accurately reflects the current reality” (Australian Human Rights Commission). Many obsolete data contain outdated language or ideals. For instance, when we looked at the Titanic data set, we see that sex contained a binary male or female. However, more up-to-date data would also consider those who are non-binary. Secondly, in the data generation category, they found five different sources of bias, including specification bias and inherited bias. Specification bias occurs when a potential independent component is excluded from the general model, resulting in a biased estimate of the coefficients. Inherited bias refers to the underlying assumptions that skew viewpoints and data. Finally, in the learning category, a way bias propagates is through inductive bias. The inductive bias of an algorithm is the set of assumptions the learner uses to predict outputs of given inputs that it has not encountered. That list is just a small, but widespread, list of ways bias presents itself in Machine Learning and algorithms.\nThese biases are reflected in the usage of algorithms in all of the examples I listed above. In finance, machine learning has been used to make predictions about creditworthiness. But what isn’t discussed is how minorities and underrepresented groups are hypothesized to have lower creditworthiness. As a result, it is harder for them to get loans, buy homes, and make investments. Machine learning has also made large advancements in healthcare. But for a while, women were given worse treatment than men because predictions were directed toward men. And POC was given worse treatment than white people because predictions and calculations were made specifically for white people. Machine learning was used in the criminal justice system to predict crime and recidivism rates, but the algorithm was so skewed that the algorithm targeted black individuals in black communities. It turned out that black individuals were given harsher sentences than white individuals for the same crime. But there are also other discriminatory algorithms. A study found that “Amazon’s recruitment tool, which produced AI-based recommendations that significantly favored men over women for technical jobs” (Questioning Racial and Gender Bias in AI-based Recommendations: Do Espoused National Cultural Values Matter?).\nStudying discrimination and prejudice in these algorithms, many researchers have concluded that these quantitative methods are incredibly problematic. One of those researchers is Arvind Narayanan. In his speech, The Limits of the Quantitative Approach to Discrimination, he argues that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Arvind Narayanan). While this is a strong statement, Arvind backs up this claim with strong evidence. In his speech, he talks about the practice of quantitative methods. More specifically, he talks about the usage of the null hypothesis. When studying racism, for instance, the null hypothesis is that there is no racism. Similarly, when studying any form of prejudice, the null hypothesis is that there is no prejudice. This is problematic because “baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (Arvind Narayanan). It’s problematic that the null hypothesis is that there is no discrimination because when there is no significant evidence that discrimination exists, people verify and justify the null hypothesis, thus seeing the status quo as something without flaws or issues. And “when researchers pick the null hypothesis on autopilot, mimicking what’s been done before, they are often oblivious to the fact that their choice has enormous normative significance” (Arvind Narayanan). There is also the issue with data. Often the data we use are snapshots from a day or a short period. But snapshots also lose people’s lived experiences because it is not recorded in the data. So, snapshots “frame discrimination as happening at discrete moments in time rather than encoded into the way that our institutions are designed” (Arvind Narayanan). In other words, the data doesn’t paint a holistic image, ignoring systematic and structural discrimination. Another problematic aspect of these quantitative methods is who produces the data. Usually, larger companies and organizations, both of which are arguably biased, gather data. So, “when companies are in control of producing data, they have simple ways of affecting the conclusions that are drawn by controlling which data are collected or released” (Arvind Narayanan).\nThese sentiments by Arvind are echoed through other research papers. Dr. Alex Hanna from Harvard University, similar to Arvind, explains that AI research is supporting ‘bad markets’ for a lack of better words. Machine learning, algorithms, and AI are controlled by “large tech companies, elite universities, or specialty labs like Open AI or Anthropic which have these really big VC term sheets and are doing things which they consider to be general-purpose AI or something of that nature” (Dr. Alex Hanna). But the reason this is bad is that it doesn’t allow researchers to be independent or community focused. Instead, it supports a capitalistic society such as corporate uses, supporting their businesses, or directed towards military and security purposes. Thus Dr. Alex Hanna concludes that AI was created for negative reasons. As an example of this, Clearview AI, a large institution, created a widely known and used facial recognition tool for law enforcement. And ShotSpotter detects things like gunshots. These tools are designed to micromanage low-income communities and communities of color. Furthermore, she explains that a lot of the problem lies in the data. She explains that the data “does not provide the full story” (Dr. Alex Hanna).\nAdrienne Yapo and Joseph Weiss of Bentley University expressed similar disdain toward algorithms and machine learning. They suggested that the most significant issue is the “black box” secrecy behind the machine learning algorithms. In other words, the algorithms are not transparent. Why? “For-profit companies that produce these algorithms do not release the criteria and calculations behind the formulas” (Yapo and Weiss). Furthermore, at times the algorithms become so complex that understanding the formulas is extremely difficult. Therefore, since the algorithms are created by humans, “they inevitably – and often unconsciously – reflect societal values, biases, and discriminatory practices” (Yapo and Weiss). Ultimately, while Yapo and Weiss find that AI can be helpful at times, reformation is needed. Inclusivity and awareness of the ethical risks and complications are crucial to the design of AI to ensure that individuals are treated fairly.\nTechnology and machine learning have come a long way. They have supported individuals in our day-to-day lives, making them important. However, the current usages of these algorithms and the foundations on which they lie are problematic. I, along with many other researchers and individuals, wouldn’t go as far as to say that the harms outweigh the benefits. There needs change and an ethical discussion, but these algorithms can still be helpful.\nSources:\nhttps://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf\nhttps://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&context=hicss-51\nhttps://arxiv.org/pdf/2004.00686.pdf\nhttps://link.springer.com/article/10.1007/s10796-021-10156-2\nhttps://www.sir.advancedleadership.harvard.edu/articles/understanding-gender-and-racial-bias-in-ai\nhttps://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems"
  },
  {
    "objectID": "posts/Perceptron/Perceptron.html",
    "href": "posts/Perceptron/Perceptron.html",
    "title": "Hello Blog",
    "section": "",
    "text": "**Quick walk through of my algorithm:\nFirst, as given to us, I start with making my blobs. I initially start with n = 100. But when I was doing my experimentation, I would change n as I changed the number of features. I wanted to have 50 samples for each feature. I also would remove the centers when I increased the number of features.\nThen I created my Perceptron class. In the indicator function I created, I returned 1 if X > 0, otherwise 0. Then I created the fit function. First, I set the weights to random numbers and I set the training score to 0. Within the for loop, I did a few calculations. I started by calculating the prediction error by doing the dot product between X and w (which are the weights plus b). I think compare y and y_pred. Based on that, I update all components of w tilde (w and b). I also update the score and the history of the scores.\nThat brings me to how I calculated the score. I score is the average of getting it right (1) vs. getting it wrong (0). I couldn’t do the score function without a prediction function. The prediction function is just the dot product of X and w. I also have the function w, which just returns the weights, and the function, history, which just turns the history of scores.\nFinally, I plot everything.\n**Experimentation:\nThe first set of experimentation that I did was changing the learning rates. Included in my fit function within my Perceptron class, I set the learning rates between 0.001 and 1. I noticed that when I changed my learning rate to 0.001, the perceptron algorithm learned slowly, much slower. However, when I set the perceptron algorithm to 1, the algorithm learned extremely fast, and I couldn’t see the progression of the accuracy. So, the sweet spot was setting the learning rate to 0.1.\nI also increeased the number of features I had to 10. When I did this, I noticed that the accuracy rate toggled in the 30’s (30% or so). This suggests that as I add features, my perceptron learns less accurately. Furthermore, when I increase the number of interations to 2,000 and maintained the number of features at 10, the accurac rate jumped to the 60’s (60% or so). Despite have a larger number of iterations, having a linearly separable graph gets more difficult. With 2 features, 1000 iterations, and a linearly separable graph, I noticed that my algorithm will always converge and produce an accuracy of 1. However, with 2 features, 1000 iterations, and a non-linearly separable graph, my algorithm never converges. The accuracy rate, at one point will stay stagnant.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n#n = (p_features - 1) * 50\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n# X, y = make_blobs(n_samples = n, n_features = p_features - 1)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nclass Perceptron:\n    def __init__(self):\n        self.weights = None\n        self.training_score = None\n        self.training_history = []\n    \n    def indicator(self, X):\n        return np.where(X > 0, 1, 0)\n    \n    def fit(self, X, y, num_iterations=1000, learning_rate=0.001):\n        self.weights = np.random.randn(X.shape[1] + 1)  # initialize weights randomly\n        print(f'Initial Weights: {self.weights}')\n\n        self.training_score = 0.0  # initialize score\n        for i in range(num_iterations):\n            y_pred = self.indicator(np.dot(X, self.weights[:-1]) + self.weights[-1])  # predict on all samples\n            # update weights based on prediction error\n            misclassified = y * y_pred <= 0\n            self.weights[:-1] += learning_rate * np.dot(X.T, y * misclassified)\n            self.weights[-1] += learning_rate * np.sum(y * misclassified)\n            # calculate score and update history\n            self.training_score = np.mean(y == y_pred)\n            self.training_history.append(self.training_score)\n\n    \n    def predict(self, X):\n        # add bias term to input\n        X = np.c_[X, np.ones(X.shape[0])]\n        return self.indicator(np.dot(X, self.weights))\n    \n    def w(self):\n        return self.weights\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        return np.mean(y == y_pred)\n    \n    def history(self):\n        return self.training_history\n\n\np = Perceptron()\np.fit(X,y, num_iterations = 1000)\n\nprint(f'Weights: {p.w()}')\nprint(f'Last 10 Training History: {p.history()[-10:]}')\nprint(f'Score: {p.score(X,y)}')\n\nplt.plot(p.history())\n\nInitial Weights: [ 1.67834508 -0.76553614  0.04903625]\nWeights: [1.46207353 0.1531806  0.72803625]\nLast 10 Training History: [0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]\nScore: 0.96\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w(), -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n**Runtime:\nI believe that the runtime of one iteration of perceptron depends on the number of features but not on the number of data points. When we do the perceptron algorithm, there is one main dot product and then we update the weights. Going through this math, I believe that the runtime would be O(p). There would be n multiplications and n additions. And because it is only one iteration, the number of n does not matter."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/TimnitGebru/TimnitGebru.html",
    "href": "posts/TimnitGebru/TimnitGebru.html",
    "title": "Hello Blog",
    "section": "",
    "text": "Timnit Gebru discussed “faception” in the 2020 Fairness Accountability Transparency and Ethics in Computer Vision lecture. What is faception? She explains that it is the “first to-technology and first-to-market with proprietary computer vision and machine learning technology for profiling people and revealing their personality based only on their facial image”. It other words, it is an algorithm that can determine your IQ or see whether or not you are a terrorist based on one facial image. This type of technology is incredibly harmful because it incorrectly categorizes people, often in a negative way, and it negatively targets minorities and those in underrepresented groups. It’s technology like this that Gebru warns us about. It’s not just faception now. For example, the Hire Vue video intelligence that detects your verbal and non-verbal cues to determine whether you are qualified for a job. It determines whether you are ‘happy’ or ‘smart’, which are all very subjective descriptions. The Baltimore police also uses extensive facial recognition in an unconstitutional way. They used facial recognition to surveil people at protests and rallies – activities protected by the First Amendment. But doing such discourages political participation. They used facial recognition on social media photos to identify people at the Freddie Gray protests and target them for unnecessary arrests.\nDigging a little bit deeper into facial recognition, Gebru discusses the usage of facial recognition in big tech corporations such as IBM and Google. She found that there were high error rates for darker-skinned women and light-skinned people. The reason for such was a lack of diversity in data sets that people use. A lot of data comes from exclusively western settings. Similarly, a lot of experiments exclusively use men. For example, in clinical trials, medicine is tested on men only. As a result, women tend to suffer more. The same is true with safety testing using dummies. A lot of the times, testing, especially for vehicle safety, use male dummies. In the real world, women are disproportionately harmed. There is also a lot of harm for transgender individuals, especially in gender recognition. There are issues with misgendering, and facial recognition also assumes that gender is binary.\nOn the bright side, there have been an increasing amount of pushback. Gebru is a part of an organization called Black in AI, which is “a place for sharing ideas, fostering collaboration, and discussing initiatives to increase the presence of Black people in the field of AI”. People are also combatting facial recognition through interesting makeup that fools facial recognition, fashion that people wear to fool facial recognition, and systems of refusal in engaging with this technology.\nAs used today, computer vision is harmful to underrepresented minorities and women because large corporations and privileged groups are using computer vision to gain power, profits, or control populations.\nA question I have for Timnit: In your 2020 lecture you explain that “there are very few people who are black, especially in the Computer Vision community”. There are also very few women. Being a woman of color in the computer vision community, have you felt imposter syndrome? If so, how do you handle such feelings?"
  },
  {
    "objectID": "posts/PenguinPost/Penguin.html",
    "href": "posts/PenguinPost/Penguin.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "train_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n#train.head()\n\n\ntrain = train[train.Sex != \".\"]\n\n\n# sns.set_theme(style=\"whitegrid\", palette = \"muted\")\n\n# ax = sns.swarmplot(data = train, x = \"Body Mass (g)\", y = \"Sex\", hue = \"Species\")\n# ax.set(ylabel = \"\")\n# plt.show()\n# sns.pairplot(train, hue= \"Species\")\n# swarm plot\nsns.scatterplot(x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\", data = train)\nplt.show()\n\n\n\n\n\ndef read_data(url):\n  df = pd.read_csv(url)\n  y = df[\"Body Mass (g)\"]\n  X = df.drop([\"Body Mass (g)\", \"Species\"], axis = 1)\n  return df, X, y\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\n\ndf_train, X_train, y_train = read_data(train_url)\n\n\n#y_train.head()\ntrain = pd.get_dummies(train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n      Sex_MALE\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      7.96621\n      -25.69327\n      NaN\n      0\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      NaN\n      0\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      NaN\n      1\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      NaN\n      1\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      NaN\n      1\n    \n  \n\n\n\n\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      7.96621\n      -25.69327\n      NaN\n      0\n      0\n    \n    \n      1\n      PAL0708\n      22\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      7.63220\n      -25.46569\n      NaN\n      1\n      0\n    \n    \n      2\n      PAL0910\n      124\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      9.59462\n      -25.42621\n      NaN\n      0\n      1\n    \n    \n      3\n      PAL0910\n      146\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      9.22033\n      -26.03442\n      NaN\n      0\n      1\n    \n    \n      4\n      PAL0708\n      24\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      9.28153\n      -24.97134\n      NaN\n      0\n      1\n    \n  \n\n\n\n\n\n# train = pd.get_dummies(train, columns = ['Sex'], drop_first = \"if_binary\")\n\ntrain.groupby('Species')[['Body Mass (g)', 'Sex_MALE']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Sex_MALE\n    \n    \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.09\n      118\n      0.47\n      118\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.86\n      56\n      0.48\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5121.97\n      100\n      0.54\n      100\n    \n  \n\n\n\n\n\nWhen looking at the species of penguins and their body mass, I can see that the Gentoo penguins are, on average, much larger in body mass than the chinstrap penguins and the adelie penguins.\n\nsns.scatterplot(data=train, x='Body Mass (g)', y='Flipper Length (mm)')\n\n<AxesSubplot: xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\n\ngrouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\ngrouped\n\n/var/folders/k5/sv5thph578dff6sqh3h3z2kw0000gn/T/ipykernel_25399/3046570246.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  grouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\n\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      median\n      std\n      mean\n      median\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.094017\n      3650.0\n      455.209898\n      189.965812\n      190.0\n      6.678493\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.857143\n      3687.5\n      404.876925\n      195.464286\n      195.0\n      7.032300\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5121.969697\n      5100.0\n      511.653391\n      217.656566\n      217.0\n      6.438344\n    \n  \n\n\n\n\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      7.96621\n      -25.69327\n      NaN\n      0\n      0\n    \n    \n      1\n      PAL0708\n      22\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      7.63220\n      -25.46569\n      NaN\n      1\n      0\n    \n    \n      2\n      PAL0910\n      124\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      9.59462\n      -25.42621\n      NaN\n      0\n      1\n    \n    \n      3\n      PAL0910\n      146\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      9.22033\n      -26.03442\n      NaN\n      0\n      1\n    \n    \n      4\n      PAL0708\n      24\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      9.28153\n      -24.97134\n      NaN\n      0\n      1\n    \n  \n\n\n\n\nDescription of the scatterplot and the table above\n\n# #for loop for features\n\n# # from sklearn.linear_model import LogisticRegression\n\n# # LR = LogisticRegression()\n# # LR.fit(X_train, y_train)\n# # LR.score(X_train, y_train)\n# from sklearn.linear_model import LogisticRegression\n\n# # this counts as 3 features because the two Clutch Completion \n# # columns are transformations of a single original measurement. \n# # you should find a way to automatically select some better columns\n# # as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Culmen Depth (mm)\", \"Sex_MALE\"]\n\n# LR = LogisticRegression()\n# LR.fit(X_train[cols], y_train)\n# LR.score(X_train[cols], y_train)\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\npair = combinations(all_quant_cols, 3)\ncols = qual_cols + list(pair)\nX_train_sub = X_train[cols]\n\n/Users/kaylynnxia/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/common.py:245: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  result = np.asarray(values, dtype=dtype)\n\n\nKeyError: \"[('Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)')] not in index\"\n\n\n\nbest_cols = []\nbest_acc = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 3):\n        cols = qual_cols + list(pair)\n        X_train_sub = X_train[cols]\n        X_test_sub = X_test[cols]\n        le = LabelEncoder()\n        y_train_enc = le.fit_transform(y_train)\n        y_test_enc = le.transform(y_test)\n        clf = RandomForestClassifier(n_estimators=100, random_state=0)\n        clf.fit(X_train_sub, y_train_enc)\n        acc = clf.score(X_test_sub, y_test_enc)\n        if acc == 1:\n            best_cols = cols\n            best_acc = acc\n            break\n    if best_acc == 1:\n        break\n\nNameError: name 'LabelEncoder' is not defined\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    # \n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\n\ndf_test, X_test, y_test = read_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Species\"], drop_first=\"if_binary\")\n\nKeyError: \"None of [Index(['Species'], dtype='object')] are in the [columns]\"\n\n\n\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score\n\n# # select the relevant columns\n# qual_col = [\"Clutch Completion\", \"Sex\"]\n# quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n# # create the feature and target arrays\n# X = train[quant_cols + [qual_col]].values\n# y = train['Species'].values\n\n# # split the data into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# def train_and_evaluate(X_train, y_train, X_test, y_test):\n#     # train the model on the training set\n#     clf = RandomForestClassifier(n_estimators=100, random_state=42)\n#     clf.fit(X_train, y_train)\n    \n#     # make predictions on the test set\n#     y_pred = clf.predict(X_test)\n    \n#     # calculate the accuracy of the model\n#     accuracy = accuracy_score(y_test, y_pred)\n    \n#     return accuracy\n\n\n# from itertools import combinations\n\n# best_accuracy = 0\n# best_features = None\n\n# for qual in [qual_col]:\n#     qual_cols = [col for col in train.columns if qual in col]\n#     for pair in combinations(quant_cols, 2):\n#         cols = qual_cols + list(pair)\n#         X_train_subset = X_train[:, [i for i, col in enumerate(train.columns) if col in cols]]\n#         X_test_subset = X_test[:, [i for i, col in enumerate(train.columns) if col in cols]]\n#         accuracy = train_and_evaluate(X_train_subset, y_train, X_test_subset, y_test)\n#         if accuracy > best_accuracy:\n#             best_accuracy = accuracy\n#             best_features = cols\n\n# print(\"Best features:\", best_features)\n# print(\"Best accuracy:\", best_accuracy)"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\n\nLR = LogisticRegression()\nLR.fit(X,y)\nplot_decision_regions(X,y,clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X,y)}\")\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\nLR2 = LogisticRegression()\n\nX_ = X**2\nLR2 = LogisticRegression();\nLR2.fit(X_, y)\nplot_decision_regions(X_, y, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_, y)}\")\n#\n#\n#\n#"
  },
  {
    "objectID": "Gradient.html",
    "href": "Gradient.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from solutions.logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nModuleNotFoundError: No module named 'solutions'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Limits of the Quantitative Approach Essay\n\n\n\n\n\n\nApr 18, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Perceptron.html",
    "href": "Perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "p = perceptron.Perceptron()\n\np.fit(X, y, 1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2"
  }
]