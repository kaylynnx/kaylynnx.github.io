[
  {
    "objectID": "posts/LimitsQuantitativeApproach/Essay.html",
    "href": "posts/LimitsQuantitativeApproach/Essay.html",
    "title": "Hello Blog",
    "section": "",
    "text": "As long as humans have existed, technology has been a powerful force. From the wheel to running water to the computer, technology has aided our day-to-day lives. In modern-day technology, since the mid-1900s, technology has expanded past new inventions to more quantitative innovations. The generation of new phones and computers has become relatively stagnant, but the usage of phones and computers to aid our day-to-day lives has exploded. Scientists and businessmen created Machine Learning, more colloquially known as Artificial Intelligence. Artificial Intelligence has been displayed in movies such as Robocop or the recently released horror movie, Megan. These movies depict a universe in which Artificial Intelligence is self-aware and relatively human. While largely exaggerated, we have created machines to learn from data, hence why it is called Machine Learning. Given some historical data or a collection of current data, the machines can generate some sort of conclusion.\nMachine Learning is used in the finance industry to uncover credit card fraud, make predictions about creditworthiness, and identify trends in the stock market (Ethical Implications of Bias in Machine Learning). Machine learning has been incredibly productive and helpful to provide important advances in health care and treatment decisions (Ethical Implications of Bias in Machine Learning). The criminal justice system is using machine learning to predict crime hotspots and recidivism rates (Ethical Implications of Bias in Machine Learning). AI and Machine learning has been increasingly widespread. They have been helpful in all aspects of our lives. But with all great innovations, there are unintended consequences. One of the unintended consequences is bias. What is bias? When we think of bias or fairness in a social, political, or economic sense, bias is defined as prejudice against one person, group, or thing in a way classified as ‘unfair’. Essentially, bias in an algorithmic sense, which we coin algorithmic bias, is very similar. Most researchers in this field define algorithmic bias differently. Nevertheless, “most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status” (Bias in Machine Learning – What is it Good For). What does this mean exactly? Well, bias in Machine Learning isn’t so black or white. Instead, the authors of Bias in Machine Learning found that biases can be organized into three distinct categories: “a biased world, data generation, and learning” (Bias in Machine Learning – What is it Good For). Within a biased world, bias may propagate through what people would refer to as historical bias. According to the Australian Human Rights Commission, “historical bias arises when the data used to train an AI system no longer accurately reflects the current reality” (Australian Human Rights Commission). Many obsolete data contain outdated language or ideals. For instance, when we looked at the Titanic data set, we see that sex contained a binary male or female. However, more up-to-date data would also consider those who are non-binary. Secondly, in the data generation category, they found five different sources of bias, including specification bias and inherited bias. Specification bias occurs when a potential independent component is excluded from the general model, resulting in a biased estimate of the coefficients. Inherited bias refers to the underlying assumptions that skew viewpoints and data. Finally, in the learning category, a way bias propagates is through inductive bias. The inductive bias of an algorithm is the set of assumptions the learner uses to predict outputs of given inputs that it has not encountered. That list is just a small, but widespread, list of ways bias presents itself in Machine Learning and algorithms.\nThese biases are reflected in the usage of algorithms in all of the examples I listed above. In finance, machine learning has been used to make predictions about creditworthiness. But what isn’t discussed is how minorities and underrepresented groups are hypothesized to have lower creditworthiness. As a result, it is harder for them to get loans, buy homes, and make investments. Machine learning has also made large advancements in healthcare. But for a while, women were given worse treatment than men because predictions were directed toward men. And POC was given worse treatment than white people because predictions and calculations were made specifically for white people. Machine learning was used in the criminal justice system to predict crime and recidivism rates, but the algorithm was so skewed that the algorithm targeted black individuals in black communities. It turned out that black individuals were given harsher sentences than white individuals for the same crime. But there are also other discriminatory algorithms. A study found that “Amazon’s recruitment tool, which produced AI-based recommendations that significantly favored men over women for technical jobs” (Questioning Racial and Gender Bias in AI-based Recommendations: Do Espoused National Cultural Values Matter?).\nStudying discrimination and prejudice in these algorithms, many researchers have concluded that these quantitative methods are incredibly problematic. One of those researchers is Arvind Narayanan. In his speech, The Limits of the Quantitative Approach to Discrimination, he argues that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Arvind Narayanan). While this is a strong statement, Arvind backs up this claim with strong evidence. In his speech, he talks about the practice of quantitative methods. More specifically, he talks about the usage of the null hypothesis. When studying racism, for instance, the null hypothesis is that there is no racism. Similarly, when studying any form of prejudice, the null hypothesis is that there is no prejudice. This is problematic because “baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (Arvind Narayanan). It’s problematic that the null hypothesis is that there is no discrimination because when there is no significant evidence that discrimination exists, people verify and justify the null hypothesis, thus seeing the status quo as something without flaws or issues. And “when researchers pick the null hypothesis on autopilot, mimicking what’s been done before, they are often oblivious to the fact that their choice has enormous normative significance” (Arvind Narayanan). There is also the issue with data. Often the data we use are snapshots from a day or a short period. But snapshots also lose people’s lived experiences because it is not recorded in the data. So, snapshots “frame discrimination as happening at discrete moments in time rather than encoded into the way that our institutions are designed” (Arvind Narayanan). In other words, the data doesn’t paint a holistic image, ignoring systematic and structural discrimination. Another problematic aspect of these quantitative methods is who produces the data. Usually, larger companies and organizations, both of which are arguably biased, gather data. So, “when companies are in control of producing data, they have simple ways of affecting the conclusions that are drawn by controlling which data are collected or released” (Arvind Narayanan).\nThese sentiments by Arvind are echoed through other research papers. Dr. Alex Hanna from Harvard University, similar to Arvind, explains that AI research is supporting ‘bad markets’ for a lack of better words. Machine learning, algorithms, and AI are controlled by “large tech companies, elite universities, or specialty labs like Open AI or Anthropic which have these really big VC term sheets and are doing things which they consider to be general-purpose AI or something of that nature” (Dr. Alex Hanna). But the reason this is bad is that it doesn’t allow researchers to be independent or community focused. Instead, it supports a capitalistic society such as corporate uses, supporting their businesses, or directed towards military and security purposes. Thus Dr. Alex Hanna concludes that AI was created for negative reasons. As an example of this, Clearview AI, a large institution, created a widely known and used facial recognition tool for law enforcement. And ShotSpotter detects things like gunshots. These tools are designed to micromanage low-income communities and communities of color. Furthermore, she explains that a lot of the problem lies in the data. She explains that the data “does not provide the full story” (Dr. Alex Hanna).\nAdrienne Yapo and Joseph Weiss of Bentley University expressed similar disdain toward algorithms and machine learning. They suggested that the most significant issue is the “black box” secrecy behind the machine learning algorithms. In other words, the algorithms are not transparent. Why? “For-profit companies that produce these algorithms do not release the criteria and calculations behind the formulas” (Yapo and Weiss). Furthermore, at times the algorithms become so complex that understanding the formulas is extremely difficult. Therefore, since the algorithms are created by humans, “they inevitably – and often unconsciously – reflect societal values, biases, and discriminatory practices” (Yapo and Weiss). Ultimately, while Yapo and Weiss find that AI can be helpful at times, reformation is needed. Inclusivity and awareness of the ethical risks and complications are crucial to the design of AI to ensure that individuals are treated fairly.\nTechnology and machine learning have come a long way. They have supported individuals in our day-to-day lives, making them important. However, the current usages of these algorithms and the foundations on which they lie are problematic. I, along with many other researchers and individuals, wouldn’t go as far as to say that the harms outweigh the benefits. There needs change and an ethical discussion, but these algorithms can still be helpful.\nSources:\nhttps://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf\nhttps://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&context=hicss-51\nhttps://arxiv.org/pdf/2004.00686.pdf\nhttps://link.springer.com/article/10.1007/s10796-021-10156-2\nhttps://www.sir.advancedleadership.harvard.edu/articles/understanding-gender-and-racial-bias-in-ai\nhttps://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems"
  },
  {
    "objectID": "posts/Perceptron/Perceptron.html",
    "href": "posts/Perceptron/Perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "**Quick walk through of my algorithm:\nFirst, as given to us, I start with making my blobs. I initially start with n = 100. But when I was doing my experimentation, I would change n as I changed the number of features. I wanted to have 50 samples for each feature. I also would remove the centers when I increased the number of features.\nThen I created my Perceptron class. In the indicator function I created, I returned 1 if X > 0, otherwise 0. Then I created the fit function. First, I set the weights to random numbers and I set the training score to 0. Within the for loop, I did a few calculations. I started by calculating the prediction error by doing the dot product between X and w (which are the weights plus b). I think compare y and y_pred. Based on that, I update all components of w tilde (w and b). I also update the score and the history of the scores.\nThat brings me to how I calculated the score. I score is the average of getting it right (1) vs. getting it wrong (0). I couldn’t do the score function without a prediction function. The prediction function is just the dot product of X and w. I also have the function w, which just returns the weights, and the function, history, which just turns the history of scores.\nFinally, I plot everything.\n**Experimentation:\nThe first set of experimentation that I did was changing the learning rates. Included in my fit function within my Perceptron class, I set the learning rates between 0.001 and 1. I noticed that when I changed my learning rate to 0.001, the perceptron algorithm learned slowly, much slower. However, when I set the perceptron algorithm to 1, the algorithm learned extremely fast, and I couldn’t see the progression of the accuracy. So, the sweet spot was setting the learning rate to 0.1.\nI also increeased the number of features I had to 10. When I did this, I noticed that the accuracy rate toggled in the 30’s (30% or so). This suggests that as I add features, my perceptron learns less accurately. Furthermore, when I increase the number of interations to 2,000 and maintained the number of features at 10, the accurac rate jumped to the 60’s (60% or so). Despite have a larger number of iterations, having a linearly separable graph gets more difficult. With 2 features, 1000 iterations, and a linearly separable graph, I noticed that my algorithm will always converge and produce an accuracy of 1. However, with 2 features, 1000 iterations, and a non-linearly separable graph, my algorithm never converges. The accuracy rate, at one point will stay stagnant.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n#n = (p_features - 1) * 50\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n# X, y = make_blobs(n_samples = n, n_features = p_features - 1)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nclass Perceptron:\n    def __init__(self):\n        self.weights = None\n        self.training_score = None\n        self.training_history = []\n    \n    def indicator(self, X):\n        return np.where(X > 0, 1, -1)\n    \n    def fit(self, X, y, num_iterations=1000, learning_rate=0.01):\n        self.weights = np.random.randn(X.shape[1] + 1)  # initialize weights randomly\n        print(f'Initial Weights: {self.weights}')\n\n        self.training_score = 0.0  # initialize score\n        for i in range(num_iterations):\n            y_pred = self.indicator(np.dot(X, self.weights[:-1]) + self.weights[-1])  # predict on all samples\n            # update weights based on prediction error\n            misclassified = y * y_pred <= 0\n            self.weights[:-1] += learning_rate * np.dot(X.T, y * misclassified)\n            self.weights[-1] += learning_rate * np.sum(y * misclassified)\n            # calculate score and update history\n            self.training_score = np.mean(y == y_pred)\n            self.training_history.append(self.training_score)\n\n    \n    def predict(self, X):\n        # add bias term to input\n        X = np.c_[X, np.ones(X.shape[0])]\n        return self.indicator(np.dot(X, self.weights))\n    \n    def w(self):\n        return self.weights\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        return np.mean(y == y_pred)\n    \n    def history(self):\n        return self.training_history\n\n\np = Perceptron()\np.fit(X,y, num_iterations = 1000)\n\nprint(f'Weights: {p.w()}')\nprint(f'Last 10 Training History: {p.history()[-10:]}')\nprint(f'Score: {p.score(X,y)}')\n\nplt.plot(p.history())\n\nInitial Weights: [ 1.67834508 -0.76553614  0.04903625]\nWeights: [1.47322257 0.1682865  0.73903625]\nLast 10 Training History: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\nScore: 0.5\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w(), -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nTesting my perceptron algorithm on linearly seperable data, we can see that my perceptron algorithm is able to correctly separate the two sets of points, as seen below:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate linearly separable data\nnp.random.seed(0)\nX = np.random.randn(100, 2)\ny = np.where(X[:,0] + X[:,1] > 0, 1, -1)\n\n# Fit the perceptron algorithm to the data\np = Perceptron()\np.fit(X, y)\n\n# Plot the decision boundary\nfig = plt.scatter(X[:,0], X[:,1], c=y)\nfig = draw_line(p.w(), -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.show()\n\nInitial Weights: [-0.36918184 -0.23937918  1.0996596 ]\n\n\n\n\n\n**Runtime:\nI believe that the runtime of one iteration of perceptron depends on the number of features but not on the number of data points. When we do the perceptron algorithm, there is one main dot product and then we update the weights. Going through this math, I believe that the runtime would be O(p). There would be n multiplications and n additions. And because it is only one iteration, the number of n does not matter."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/TimnitGebru/TimnitGebru.html",
    "href": "posts/TimnitGebru/TimnitGebru.html",
    "title": "Hello Blog",
    "section": "",
    "text": "On April 24th, Timnit Gebru will be visiting Middlebury College to give a presentation on bias and social impacts of AI. Born and raised in Ethiopia, Timnit is an American computer scientist who works on algorithmic biases and data mining. She has been a vocal advocate for diversity in technology, she is the co-founder of Black in AI, a community of Black researchers working in AI, and she is the founder of the Distributed Artificial Intelligence Research Institute (DAIR). Timnit had a rough childhood ranging from her father’s death, feeling the Eritrean-Ethiopian war, being denied entry to the United States, and facing systematic racism in her early years of living in the United States. Her experiences led her to ethics in technology. In 2001, Timnit was accepted at Stanford University where she earned her Bachelor of Science and Master of Science degrees in electrical engineering and her PhD in computer vision. Shortly after her completing of her PhD, she joined Google where she co-led a team on the ethics of artificial intelligence. She studied the implications of artificial intelligence, looking to improve the ability of technology to do social good. However, just two years into her role, in December 2020, Gebru controversially departed from Google. Timnit had published a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” that covered the risks of large language models. In December 2020, Gebru’s employment ended after high Google managers asked her to either withdraw the paper or remove the names of all the Google employees from the paper. Gebru requested insight into their decision and warned that non-compliance would result in her departure. Google terminated her employment immediately, stating they were accepting her resignation. Aside from this controversy, Timnit has been widely recognized. She was named one of the World’s 50 Greatest Leaders by Fortune, won the 2019 AI Innovations Award in the category AI for Good for highlighting the significant problem of algorithmic bias in facial recognition, one of Nature’s ten people who shaped science in 2021, and one of Time’s most influential people in 2022.\nTimnit Gebru discussed “faception” in the 2020 Fairness Accountability Transparency and Ethics in Computer Vision lecture. What is faception? She explains that it is the “first to-technology and first-to-market with proprietary computer vision and machine learning technology for profiling people and revealing their personality based only on their facial image”. It other words, it is an algorithm that can determine your IQ or see whether or not you are a terrorist based on one facial image. This type of technology is incredibly harmful because it incorrectly categorizes people, often in a negative way, and it negatively targets minorities and those in underrepresented groups. It’s technology like this that Gebru warns us about. It’s not just faception now. For example, the Hire Vue video intelligence that detects your verbal and non-verbal cues to determine whether you are qualified for a job. It determines whether you are ‘happy’ or ‘smart’, which are all very subjective descriptions. The Baltimore police also uses extensive facial recognition in an unconstitutional way. They used facial recognition to surveil people at protests and rallies – activities protected by the First Amendment. But doing such discourages political participation. They used facial recognition on social media photos to identify people at the Freddie Gray protests and target them for unnecessary arrests. Digging a little bit deeper into facial recognition, Gebru discusses the usage of facial recognition in big tech corporations such as IBM and Google. She found that there were high error rates for darker-skinned women and light-skinned people. The reason for such was a lack of diversity in data sets that people use. A lot of data comes from exclusively western settings. Similarly, a lot of experiments exclusively use men. For example, in clinical trials, medicine is tested on men only. As a result, women tend to suffer more. The same is true with safety testing using dummies. A lot of the times, testing, especially for vehicle safety, use male dummies. In the real world, women are disproportionately harmed. There is also a lot of harm for transgender individuals, especially in gender recognition. There are issues with misgendering, and facial recognition also assumes that gender is binary. On the bright side, there have been an increasing amount of pushback. Gebru is a part of an organization called Black in AI, which is “a place for sharing ideas, fostering collaboration, and discussing initiatives to increase the presence of Black people in the field of AI”. People are also combatting facial recognition through interesting makeup that fools facial recognition, fashion that people wear to fool facial recognition, and systems of refusal in engaging with this technology.\nAs used today, computer vision is harmful to underrepresented minorities and women because large corporations and privileged groups are using computer vision to gain power, profits, or control populations.\nA question I have for Timnit: In your 2020 lecture you explain that “there are very few people who are black, especially in the Computer Vision community”. There are also very few women. Being a woman of color in the computer vision community, have you felt imposter syndrome? If so, how do you handle such feelings?   Part 2:\nWe listened to Timnit Gebru’s lecture on “Eugenics and the Promise of Utopia through Artificial General Intelligence”. In her discussion, she starts by explaining how many people are not getting rich from AI. Contrary to popular beliefs, creating well-functioning and modern AI require exploited workers. Their job is to supply data without getting compensation. For instance, Kenyan workers are paid less than $2 per hour to moderate explicit and violent content to help produce work like ChatGPT. So, when wealthy individuals talk about the benefits of AI, they fail to acknowledge the harms to underprivileged people and exploited employees.\nFurthermore, Gebru connects AGI to Eugenics. Starting with the discussion of AGI, Gebru explains how the definition of AGI is not well-defined. When people attempt to discuss AGI, they explain AGI as an autonomous system that can outperform and out-think humans. It almost sounds like God. And this way of thinking is rooted in the 20th century Anglo-American thinking, such as Eugenics.\nWhen thinking about Eugenics, many people connect the movement to Nazis and believe that eugenics disappeared at the end of World War II. However, eugenics continue. We can split eugenics into three waves, but Gebru focuses on the first two. In the first wave, the goal of eugenics was to “improve the human stock through negative eugenics”. We can define negative eugenics as the attempt to get rid of undesirable traits because it would help improve the overall human race. Undesirable traits include idiocrasy, specific skin colors, disabilities, etc. This caused discrimination against under-represented groups. And we saw this will the mass genocide of Jews during World War II. After the first wave of eugenics, which died at the end of WWII, we entered the second wave of eugenics. The goal of the second wave was to improve the human stock through positive eugenics. This includes giving people the ability to design their children or encouraging people with desirable traits to reproduce more. But within Eugenics, we see people strive to “transcend humanism”, emerge with technology, and explode with intelligence.\nWe see this goal arise with AGI as well. The goal of many people is to enhance the human race and grow beyond what humans are capable of. And this leads us to the TESCREAL bundle. AGI can lead to utopia, or it can lead to an apocalypse. Starting with AGI utopia, we saw the idea of utopia emerge during the first and second wave eugenics. But in short, if we produce the necessary technology, we can bloom and thrive. On the other hand, technology can go wrong and lead to an apocalypse. People often oversimplify technology and the future of technology by classifying it as either a utopia or an apocalypse. In reality, AGI has started a race to the bottom. Every company feels like they have to create a certain model that can do everything. And this leads to a lot of centralization of power. This makes it difficult for small organizations to get funding for projects and products. Also, it is often the case that larger companies do subpar work compared to smaller companies. This debunks the idea of AGI Utopia. In terms of AGI Apocalypse, Gebru explains that by talking about at apocalypse, which is much more likely than a utopia, we are detracting accountability from the corporations that are building the machines. Instead, we are placing the blame on the machines itself.\nGiven this talk, I find that there is accountability that needs to happen, especially within large corporations. The exploitation of workers and the race to the bottom creates dishonesty and a lack of transparency. While there are many points that I agree with, I found Gebru’s talk to be lacking in a lot of ways. The connection between Eugenics and AGI felt like a large leap. There are many goals and ideas that are shared between the communities, but I do not understand the connection between the two. I also found her talk to be lacking in concrete evidence. That also made me skeptical about her lecture. I would also like to discuss her reaction to challenges and disagreements in her argument. The question-and-answer portion of her lecture seemed unprofessional. When presented with a question that opposed her opinions, she immediately shut the audience member down, exclaiming that it was inappropriate to ask such questions. That discredited her even more because having a strong understanding in a topic suggests that you understand both sides of an argument. Having a professional conversation about a controversial topic is much more beneficial and persuasive if the conversation presents facts and is calm. The fact that she refused to hear and answer someone’s question that challenges her beliefs partially suggests that she lacks the understanding of the other side.\nI thoroughly enjoyed hearing about her background in our in-class portion. She paved the way for many POC women in computer science. And her experiences are inspiring. However, I was set back by the lecture aspect. I understand that she’s extremely passionate about eugenics and AIG. But after that lecture, I am skeptical about her research."
  },
  {
    "objectID": "posts/PenguinPost/Penguin.html",
    "href": "posts/PenguinPost/Penguin.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "---\ntitle: Hello Blog\nauthor: Kaylynn Xia\ndate: '2023-04-25'\nimage: \"image.jpg\"\ndescription: \"My Classifying Pengiuns Blog Post.\"\nformat: html\n---\n\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nsns.scatterplot(x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\", data = train)\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\nraw_data = pd.read_csv(train_url)\nX_train, y_train = prepare_data(raw_data)\n\n\ntrain = pd.get_dummies(train, columns = ['Sex'], drop_first = \"if_binary\")\n\ntrain.groupby('Species')[['Body Mass (g)', 'Sex_MALE']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Sex_MALE\n    \n    \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.09\n      118\n      0.47\n      118\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.86\n      56\n      0.48\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5119.50\n      101\n      0.53\n      101\n    \n  \n\n\n\n\n\nWhen looking at the species of penguins and their body mass, I can see that the Gentoo penguins are, on average, much larger in body mass than the chinstrap penguins and the adelie penguins.\n\nsns.scatterplot(data=train, x='Body Mass (g)', y='Flipper Length (mm)')\n\n<AxesSubplot:xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\n\ngrouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\ngrouped\n\n/var/folders/k5/sv5thph578dff6sqh3h3z2kw0000gn/T/ipykernel_43467/3046570246.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  grouped = train.groupby('Species')['Body Mass (g)', 'Flipper Length (mm)'].aggregate(['mean', 'median', 'std'])\n\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      median\n      std\n      mean\n      median\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      3667.094017\n      3650.0\n      455.209898\n      189.965812\n      190.0\n      6.678493\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.857143\n      3687.5\n      404.876925\n      195.464286\n      195.0\n      7.032300\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5119.500000\n      5100.0\n      509.661454\n      217.650000\n      217.0\n      6.406081\n    \n  \n\n\n\n\nDescription of the scatterplot and the table above:\nLooking at the scatter plot, we see that there is a positive correlation between flipper length and body mass. As flipper length increases, the penguin’s body mass does as well.\nLooking at the table, we can see that positive correlation between flipper length and body mass. We can also see that the Adelie Penguin has the smallest body mass and the smallest flipper length among the group. Gentoo Penguins, on the other hand, have the largest body mass and the largest flipper length among the group.\n\nfrom itertools import combinations\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nbest_cols = []\nbest_acc = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n        X_train_sub = X_train[cols]\n        clf = RandomForestClassifier(n_estimators=100, random_state=0)\n        clf.fit(X_train_sub, y_train)\n        y_pred = clf.predict(X_train_sub)\n        acc = accuracy_score(y_pred, y_train)\n        if(acc > best_acc):\n            best_acc = acc\n            best_cols = cols\n\nprint(\"best accuracy:\", best_acc)\nprint(\"best columns:\", best_cols)\n\nbest accuracy: 1.0\nbest columns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_train_sub = X_train[best_cols]\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train_sub, y_train)\n\nX_test, y_test = prepare_data(test)\ny_pred = clf.predict(X_test[best_cols])\nacc = accuracy_score(y_pred, y_test)\n\nprint(\"accuracy:\", acc)\n\naccuracy: 0.9705882352941176\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\n\nplot_regions(clf, X_test[best_cols], y_test)"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\n\nLR = LogisticRegression()\nLR.fit(X,y)\nplot_decision_regions(X,y,clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X,y)}\")\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\nLR2 = LogisticRegression()\n\nX_ = X**2\nLR2 = LogisticRegression();\nLR2.fit(X_, y)\nplot_decision_regions(X_, y, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_, y)}\")\n#\n#\n#\n#"
  },
  {
    "objectID": "Gradient.html",
    "href": "Gradient.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from solutions.logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nModuleNotFoundError: No module named 'solutions'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Limits of the Quantitative Approach Essay\n\n\n\n\n\n\nApr 18, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nKaylynn Xia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Perceptron.html",
    "href": "Perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "p = perceptron.Perceptron()\n\np.fit(X, y, 1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2"
  },
  {
    "objectID": "posts/LinearRegression/Untitled.html",
    "href": "posts/LinearRegression/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "n_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.w = None\n        self.score_history = []\n    \n    def pad(self, X):\n        return np.append(X, np.ones((X.shape[0], 1)), 1)\n    \n    def fit_analytic(self, X, y):\n        X = self.pad(X)\n        self.w = np.linalg.inv(X.T @ X) @ X.T @ y\n    \n    def fit_gradient(self, X, y, alpha, max_iter):\n        n, p = X.shape\n        self.w = np.random.randn(p)\n        XT_X = X.T @ X\n        XT_y = X.T @ y\n        for _ in range(max_iter):\n            y_pred = self.predict(X)\n            self.w -= alpha * (XT_X @ self.w - XT_y)\n            self.score_history.append(self.score(X, y))\n            \n    def predict(self, X):\n        X = self.pad(X)\n        return X @ self.w\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        ssr = np.sum((y_pred - y.mean())**2)\n        sst = np.sum((y - y.mean())**2)\n        return ssr / sst\n\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.4972\nValidation score = 0.591\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e2)\nLR2.w\n\nTypeError: 'float' object cannot be interpreted as an integer\n\n\n\nplt.plot(LR2.score(X_train, y_train))\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
  }
]