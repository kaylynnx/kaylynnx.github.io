<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kaylynn Xia">
<meta name="dcterms.date" content="2023-05-15">
<meta name="description" content="Limits of the Quantitative Approach">

<title>My Awesome CSCI 0451 Blog - Hello Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
    }
    </style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Hello Blog</h1>
                  <div>
        <div class="description">
          Limits of the Quantitative Approach
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Kaylynn Xia </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 15, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>As long as humans have existed, technology has been a powerful force. From the wheel to running water to the computer, technology has aided our day-to-day lives. In modern-day technology, since the mid-1900s, technology has expanded past new inventions to more quantitative innovations. The generation of new phones and computers has become relatively stagnant, but the usage of phones and computers to aid our day-to-day lives has exploded. Scientists and businessmen created Machine Learning, more colloquially known as Artificial Intelligence. Artificial Intelligence has been displayed in movies such as Robocop or the recently released horror movie, Megan. These movies depict a universe in which Artificial Intelligence is self-aware and relatively human. While largely exaggerated, we have created machines to learn from data, hence why it is called Machine Learning. Given some historical data or a collection of current data, the machines can generate some sort of conclusion.</p>
<p>Machine Learning is used in the finance industry to uncover credit card fraud, make predictions about creditworthiness, and identify trends in the stock market (<span class="citation" data-cites="Yapo_Weiss_2018">Yapo and Weiss (<a href="#ref-Yapo_Weiss_2018" role="doc-biblioref">2018</a>)</span>). Machine learning has been incredibly productive and helpful to provide important advances in health care and treatment decisions (<span class="citation" data-cites="Yapo_Weiss_2018">Yapo and Weiss (<a href="#ref-Yapo_Weiss_2018" role="doc-biblioref">2018</a>)</span>). The criminal justice system is using machine learning to predict crime hotspots and recidivism rates (<span class="citation" data-cites="Yapo_Weiss_2018">Yapo and Weiss (<a href="#ref-Yapo_Weiss_2018" role="doc-biblioref">2018</a>)</span>). AI and Machine learning has been increasingly widespread. They have been helpful in all aspects of our lives. But with all great innovations, there are unintended consequences. One of the unintended consequences is bias. What is bias? When we think of bias or fairness in a social, political, or economic sense, bias is defined as prejudice against one person, group, or thing in a way classified as ‘unfair’. Essentially, bias in an algorithmic sense, which we coin algorithmic bias, is very similar. Most researchers in this field define algorithmic bias differently. Nevertheless, “most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status” (Hellstrom_Bensch_Dignum_2020). What does this mean exactly? Well, bias in Machine Learning isn’t so black or white. Instead, the authors of Bias in Machine Learning found that biases can be organized into three distinct categories: “a biased world, data generation, and learning” (Hellstrom_Bensch_Dignum_2020). Within a biased world, bias may propagate through what people would refer to as historical bias. According to the Australian Human Rights Commission, “historical bias arises when the data used to train an AI system no longer accurately reflects the current reality” (<span class="citation" data-cites="The_Australian_Human_Rights_Commission_1970"> (<a href="#ref-The_Australian_Human_Rights_Commission_1970" role="doc-biblioref">1970</a>)</span>). Many obsolete data contain outdated language or ideals. For instance, when we looked at the Titanic data set, we see that sex contained a binary male or female. However, more up-to-date data would also consider those who are non-binary. Secondly, in the data generation category, they found five different sources of bias, including specification bias and inherited bias. Specification bias occurs when a potential independent component is excluded from the general model, resulting in a biased estimate of the coefficients. Inherited bias refers to the underlying assumptions that skew viewpoints and data. Finally, in the learning category, a way bias propagates is through inductive bias. The inductive bias of an algorithm is the set of assumptions the learner uses to predict outputs of given inputs that it has not encountered. That list is just a small, but widespread, list of ways bias presents itself in Machine Learning and algorithms.</p>
<p>Studying discrimination and prejudice in these algorithms, many researchers have concluded that these quantitative methods are incredibly problematic. One of those researchers is Arvind Narayanan. In his speech, The Limits of the Quantitative Approach to Discrimination, he argues that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (<span class="citation" data-cites="Narayanan">Narayanan (<a href="#ref-Narayanan" role="doc-biblioref">n.d.</a>)</span>). While this is a strong statement, Narayanan backs up this claim with strong evidence. In his speech, he talks about the practice of quantitative methods. More specifically, he talks about the usage of the null hypothesis. When studying racism, for instance, the null hypothesis is that there is no racism. Similarly, when studying any form of prejudice, the null hypothesis is that there is no prejudice. This is problematic because “baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (<span class="citation" data-cites="Narayanan">Narayanan (<a href="#ref-Narayanan" role="doc-biblioref">n.d.</a>)</span>). It’s problematic that the null hypothesis is that there is no discrimination because when there is no significant evidence that discrimination exists, people verify and justify the null hypothesis, thus seeing the status quo as something without flaws or issues. And “when researchers pick the null hypothesis on autopilot, mimicking what’s been done before, they are often oblivious to the fact that their choice has enormous normative significance” (<span class="citation" data-cites="Narayanan">Narayanan (<a href="#ref-Narayanan" role="doc-biblioref">n.d.</a>)</span>). There is also the issue with data. Often the data we use are snapshots from a day or a short period. But snapshots also lose people’s lived experiences because it is not recorded in the data. So, snapshots “frame discrimination as happening at discrete moments in time rather than encoded into the way that our institutions are designed” (<span class="citation" data-cites="Narayanan">Narayanan (<a href="#ref-Narayanan" role="doc-biblioref">n.d.</a>)</span>). In other words, the data doesn’t paint a holistic image, ignoring systematic and structural discrimination. Another problematic aspect of these quantitative methods is who produces the data. Usually, larger companies and organizations, both of which are arguably biased, gather data. So, “when companies are in control of producing data, they have simple ways of affecting the conclusions that are drawn by controlling which data are collected or released” (<span class="citation" data-cites="Narayanan">Narayanan (<a href="#ref-Narayanan" role="doc-biblioref">n.d.</a>)</span>). Therefore, Narayanan would argue that the current quantitative methods for assessing discrimination and bias justify the status quo.</p>
<p>Let’s look a little bit more closely at the quantitative methods that assess discrimination and bias. In this article, Weight Bias Among Health Professionals Specializing in Obesity, researchers looked at “anti-fat bias in health professionals” (<span class="citation" data-cites="Schwartz_Chambliss_Brownell_Blair_Billington_2003">Schwartz et al. (<a href="#ref-Schwartz_Chambliss_Brownell_Blair_Billington_2003" role="doc-biblioref">2003</a>)</span>). Anti-fat has been a more modern way of thinking. Lately, our culture has praised the ultra-skinny but shames those who are bigger in size. Among those who echo these bias sentiments are unfortunately health care professionals. While not all of them are anti-fat, there remain some who are, which decreases the standard of care. This can ruin people’s lives. In this study, the researchers found that “the obesity specialists in the study exhibited a significant implicit anti-fat bias” (<span class="citation" data-cites="Schwartz_Chambliss_Brownell_Blair_Billington_2003">Schwartz et al. (<a href="#ref-Schwartz_Chambliss_Brownell_Blair_Billington_2003" role="doc-biblioref">2003</a>)</span>). Often times, the healthcare professionals would assign the words “lazy, stupid, and worthless” to the overweight people. However, they explain that “people who work directly with obese patients exhibited less anti-fat bias” (<span class="citation" data-cites="Schwartz_Chambliss_Brownell_Blair_Billington_2003">Schwartz et al. (<a href="#ref-Schwartz_Chambliss_Brownell_Blair_Billington_2003" role="doc-biblioref">2003</a>)</span>). Looking at bias in this context is very important because there are real implications associated with anti-fat biases. How do these factors impact healthcare standards? How does this affect people’s perception of the doctors and do those perceptions affect the rate at which people are seeking medical attention? Investigating these biases are very beneficial. It’s important that people, especially healthcare workers, understand their implicit biases so that they can better treat patients from different backgrounds.</p>
<p>Furthermore, we can look closer at algorithmic auditing. These audits are used to help “expose systematic biases embedded in software platforms” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>). There are many audits that researchers use to evaluate discrimination and biases in algorithms. And these targeted algorithmic audits “provide one mechanism to incentivize corporations to address the algorithmic bias present in data-centric technologies that continue to play an integral role in daily life, from governing access to information and economic opportunities to influencing personal freedoms” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>). These algorithmic audits are incredibly helpful to review potential discrimination found in large corporations and algorithms. For instance, there was a study that found that “Amazon’s recruitment tool, which produced AI-based recommendations that significantly favored men over women for technical jobs” (<span class="citation" data-cites="Gupta_Parra_Dennehy_2021">Gupta, Parra, and Dennehy (<a href="#ref-Gupta_Parra_Dennehy_2021" role="doc-biblioref">2021</a>)</span>). An algorithmic audit on this would have been very informative and helpful. These audits focus on “user awareness of algorithmic bias” and “evaluate the impact of bias on user behavior and outcomes” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>). By spreading awareness of these widespread algorithms, built by monopolized tech corporations, people can better understand the harms surrounding the algorithms. More specifically, to audit commercial systems, such as Amazon’s recruitment tool, researchers will use a ‘black box audit’, where “the direct or indirect influence of input features on classifier accuracy or outcomes is inferred through the evaluation of a curated benchmark” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>). When using this audit, the vendor names are kept anonymous, and the scope is scaled down to a single named target. This means that it rarely provokes public pressure and reduces the impetus for corporate reactions. Regardless, we are able to better ourselves by understanding the implications of big tech algorithms.</p>
<p>Similar to the black box audit, there is also the gender shades study. It is an “external and multi-target black box audit of commercial machine learning” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>). Within this audit, they test “attributes of gender, reduced to the binary categories of male or female, as well as binary Fitzpatrick score, a numerical classification schema for human skin type, evaluated by a dermatologist, and grouped into classes of lighter and darker skin types” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>). This audit seems problematic. By reducing gender to a binary male or female, we ignore those who do not fit into either category such as binary or transgender people. Furthermore, there are similar issues presented when we look at skin color as ‘lighter’ or ‘darker’. However, when using the gender shades study to evaluate companies, the results were very promising. The researchers from this study found that “by highlighting the issue of classification performance disparities and amplifying public awareness, the study was able to motivate companies to prioritize the issue and yield significant improvements within 7 months” (<span class="citation" data-cites="Raji_Buolamwini_2019">Raji and Buolamwini (<a href="#ref-Raji_Buolamwini_2019" role="doc-biblioref">2019</a>)</span>).</p>
<p>Looking at the sources, it seems like the quantitative methods of assessing discrimination and bias partially follows the status quo. However, I would argue that they do more good than harm. It is important to consider the intentions behind these quantitative methods. Take the Gender Shades Study, for instance. It is highly problematic that they label gender as a binary female and male and classify skin color as lighter or darker. However as a result, we are bringing public awareness to an issue. People can then make decisions after reading and understanding the data.</p>
<p>Technology and machine learning have come a long way. They have supported individuals in our day-to-day lives, making them important. However, the current usages of these algorithms and the foundations on which they lie are problematic. When looking at the algorithms, people use quantitative methods to assess the discrimination and bias. I, along with many other researchers and individuals, wouldn’t go as far as to say that the harms outweigh the benefits. There needs change and an ethical discussion, but these quantitative methods can still be helpful.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-The_Australian_Human_Rights_Commission_1970" class="csl-entry" role="doc-biblioentry">
1970. <em>The Australian Human Rights Commission</em>. <a href="https://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems">https://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems</a>.
</div>
<div id="ref-Gupta_Parra_Dennehy_2021" class="csl-entry" role="doc-biblioentry">
Gupta, Manjul, Carlos M. Parra, and Denis Dennehy. 2021. <span>“Questioning Racial and Gender Bias in AI-Based Recommendations: Do Espoused National Cultural Values Matter? - Information Systems Frontiers.”</span> <em>SpringerLink</em>. Springer US. <a href="https://link.springer.com/article/10.1007/s10796-021-10156-2">https://link.springer.com/article/10.1007/s10796-021-10156-2</a>.
</div>
<div id="ref-Narayanan" class="csl-entry" role="doc-biblioentry">
Narayanan, Arvind. n.d. <span>“The Limits of the Quantitative Approach to Discrimination - 2022 James Baldwin Lecture, Princeton University.”</span> <em>Princeton University</em>. The Trustees of Princeton University. <a href="https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/">https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/</a>.
</div>
<div id="ref-Raji_Buolamwini_2019" class="csl-entry" role="doc-biblioentry">
Raji, Inioluwa Deborah, and Joy Buolamwini. 2019. <span>“Actionable Auditing.”</span> <em>Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</em>. <a href="https://doi.org/10.1145/3306618.3314244">https://doi.org/10.1145/3306618.3314244</a>.
</div>
<div id="ref-Schwartz_Chambliss_Brownell_Blair_Billington_2003" class="csl-entry" role="doc-biblioentry">
Schwartz, Marlene B., Heather O’Neal Chambliss, Kelly D. Brownell, Steven N. Blair, and Charles Billington. 2003. <span>“Weight Bias Among Health Professionals Specializing in Obesity.”</span> <em>Obesity Research</em> 11 (9): 1033–39. <a href="https://doi.org/10.1038/oby.2003.142">https://doi.org/10.1038/oby.2003.142</a>.
</div>
<div id="ref-Yapo_Weiss_2018" class="csl-entry" role="doc-biblioentry">
Yapo, Adrienne, and Joseph Weiss. 2018. <em>Ethical Implications of Bias in Machine Learning</em>. <a href="https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&amp;amp;context=hicss-51">https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&amp;amp;context=hicss-51</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>