<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kaylynn Xia">
<meta name="dcterms.date" content="2023-05-10">
<meta name="description" content="Description of everything we accomplished for our project">

<title>My Awesome CSCI 0451 Blog - Hello Blog - Project Blog Post</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
    }
    </style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Hello Blog - Project Blog Post</h1>
                  <div>
        <div class="description">
          Description of everything we accomplished for our project
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Kaylynn Xia </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 10, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<ol type="1">
<li>Abstract</li>
</ol>
<p>Human Activity Recognition (HAR) is the act of identifying and naming activities given some raw data through devices. You can think of a smart watch tracking how many steps you take in a day. You can think of surveilling people based on their walking patterns. HAR is a relatively new concept and has not yet been perfected. Different researchers have used different algorithms to determine what the best algorithm is. We wanted to take three algorithms – K-Nearest Neighbors, Multilayer Perceptron, which is also known as a Fully Connected Neural Network, and Random Forest Classifier – and test it on our HAR data to see which one is actually the best. We found that a K-Nearest Neighbors algorithm produced the best results, achieving an accuracy score of nearly 90%. The neural network and the random forest classifier produced similar results, achieving an accuracy score of 85% and 84%.</p>
<p>Link to our GitHub repository: https://github.com/kaylynnx/MLProject/blob/main/Code.ipynb</p>
<ol start="2" type="1">
<li>Introduction</li>
</ol>
<p>In 2006, Pirrttikangas et al.&nbsp;pioneered the research on Human Activity Recognition. In their research, they “tested a model that used several multilayer perceptron and k-nearest neighbor’s algorithms to recognize 17 activities to achieve an overall accuracy of 90.61%” (<span class="citation" data-cites="Papaleonidas_Psathas_Iliadis_2021">Papaleonidas, Psathas, and Iliadis (<a href="#ref-Papaleonidas_Psathas_Iliadis_2021" role="doc-biblioref">2021</a>)</span>). Since then, there have been a multitude of papers laying out different algorithms used to train HAR data. Some algorithms achieved significantly better results than others. To name just a few, Casale et al.&nbsp;in 2011 “applied a random forest classification algorithm to model five distinct activities (walking, climbing stair, talking to a person, standing, and working on the computer)” (<span class="citation" data-cites="Papaleonidas_Psathas_Iliadis_2021">Papaleonidas, Psathas, and Iliadis (<a href="#ref-Papaleonidas_Psathas_Iliadis_2021" role="doc-biblioref">2021</a>)</span>). In 2013, Ahmen and Loutfi used “case-based reasoning, support vector machines (SVMs) and neural networks (NN) to achieve an overall accuracy of 0.86, 0.62, and 0.59 respectively” (<span class="citation" data-cites="Papaleonidas_Psathas_Iliadis_2021">Papaleonidas, Psathas, and Iliadis (<a href="#ref-Papaleonidas_Psathas_Iliadis_2021" role="doc-biblioref">2021</a>)</span>). And in 2018, Brophy et al.&nbsp;“proposed a hybrid convolutional neural network and an SVM model with an accuracy of 92.3% for four activities (walking and running on a treadmill, low and high resistance bike exercise)” (<span class="citation" data-cites="Papaleonidas_Psathas_Iliadis_2021">Papaleonidas, Psathas, and Iliadis (<a href="#ref-Papaleonidas_Psathas_Iliadis_2021" role="doc-biblioref">2021</a>)</span>).</p>
<p>Circling back to the 2006 study, another research paper published in 2021, Human Activity Recognition Using K-Nearest Neighbor Machine Learning Algorithm, presented a k-nearest neighbor algorithm for classification of human activities, namely laying, walking downstairs, sitting, walking upstairs, standing, and walking. They found that the results prove, “a high performance in the classification of human activities” (<span class="citation" data-cites="Mohsen_Elkaseer_Scholz_2021">Mohsen, Elkaseer, and Scholz (<a href="#ref-Mohsen_Elkaseer_Scholz_2021" role="doc-biblioref">2021</a>)</span>). They evaluated the accuracy score using a few different metrics and found the “weighted average precision, F1-score, and the area under the micro-average precision-recall curve for the KNN are 90.96%, 90.46%, 90.37%, and 96.5%, respectively, while the area under the ROC curve is 100%” (<span class="citation" data-cites="Mohsen_Elkaseer_Scholz_2021">Mohsen, Elkaseer, and Scholz (<a href="#ref-Mohsen_Elkaseer_Scholz_2021" role="doc-biblioref">2021</a>)</span>). As a result, these authors claim that a lot of literature has attempted to classify human activities, but the k-nearest neighbors algorithm has “shown the highest potential to address the accuracy issue in HAR” (<span class="citation" data-cites="Mohsen_Elkaseer_Scholz_2021">Mohsen, Elkaseer, and Scholz (<a href="#ref-Mohsen_Elkaseer_Scholz_2021" role="doc-biblioref">2021</a>)</span>).</p>
<ol start="3" type="1">
<li>Value Statement</li>
</ol>
<p>In most cases, HAR is used for personal care. This includes care to the elderly, medicine, and self-improvement, which includes exercise. When it comes to medicine and care, HAR is certainly beneficial. However, with every great idea, there are usages that are a cause for concern. With that in mind, there are a lot of people – young, old, healthy, and unhealthy – that would benefit from HAR. Medicine can improve, people can track their fitness, and everyone can use it as a good source of data. (<span class="citation" data-cites="Gupta_Gupta_Pathak_Jain_Rashidi_Suri_2022">Gupta et al. (<a href="#ref-Gupta_Gupta_Pathak_Jain_Rashidi_Suri_2022" role="doc-biblioref">2022</a>)</span>)</p>
<p>However, recently, HAR has been used for surveillance and security. In the article titled, Vision-based Human Activity Recognition: A Survey, the researchers found that HAR has been used for surveillance and security. When facial recognition isn’t present, human recognition can be used to target people based on things like posture and walking pattern (<span class="citation" data-cites="Beddiar_Nini_Sabokrou_Hadid_2020">Beddiar et al. (<a href="#ref-Beddiar_Nini_Sabokrou_Hadid_2020" role="doc-biblioref">2020</a>)</span>). This is obviously very controversial. Such a tool can be used to target minorities and people of color. It could have target those who are innocent. In this sense, many underrepresented people would suffer.</p>
<ol start="4" type="1">
<li>Materials and Methods</li>
</ol>
<p>We used the dataset from the UCI Machine Learning Repository. This dataset includes 30 volunteers between the ages of 19 and 48 who wore a Samsung Galaxy phone on their waist and were requested to perform a series of six activities. These activities are standing, sitting, laying, walking, walking downstairs, and walking upstairs. The phone calculated movement in the x-axis, y-axis, and z-axis. The creators of the dataset were able to extract just total acceleration, body acceleration, and body gyroscope in the x-axis, y-axis, and z-axis. Furthermore, the dataset was pre-split into the training data and the testing data. 70% of the dataset (21 subjects) were used for the training data, and the rest (9 subjects) were used for the testing data. The data set can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip (<span class="citation" data-cites="Reyes-Ortiz_Ghio_Oneto_Anguita_Parra_2013">Reyes-Ortiz et al. (<a href="#ref-Reyes-Ortiz_Ghio_Oneto_Anguita_Parra_2013" role="doc-biblioref">2013</a>)</span>).</p>
<p>It is also important to note the limitations from the UCI Machine Learning Dataset. The dataset only includes 30 volunteers. Regardless of who these volunteers are, it is not enough people to be a good representation of the overall public. The small number of volunteers is also cause for bias. This leads us to the fact that the volunteers are between the ages 19-48. This is a young and skillful population, which could cause some inaccuracies, especially if we use our code to classify young children or the elderly.</p>
<p>For our method, we wanted to take the article about the K-nearest neighbors’ algorithm being the best algorithm for classification and either prove or disprove it. So, we knew that we wanted to train our data using the KNN algorithm. Furthermore, both the multilayer perceptron (also known as a fully connected neural network) and the random forest classifier algorithms resulted in a very good accuracy score. To test out the theory that KNN is the best algorithm, we ran this algorithm against the two other algorithms. To evaluate performance, we looked at the accuracy score, as calculated by sklearn.</p>
<ol start="5" type="1">
<li>Results</li>
</ol>
<p>We outputted 2 distinct graphs first. When we look at the first chart, we can see total acceleration, body acceleration, and body gyroscope in the x-, y-, and z-axes for a single subject. The first subject just happens to be subject 9 and the second is subject 19. These are random. When you look at this chart, we can see that there is a lot more movement around activities 1, 2, and 3, which are the walking activities. This is walking, walking upstairs, and walking downstairs. Similarly, there is a lot less movement and acceleration during activities 4, 5, and 6 because those are the static movements (sitting, standing, laying). We can also see that for the two-subject displayed, the patterns are very similar. The subjects do the same activities at the same time, and the corresponding acceleration and gyroscope movements are roughly the same.</p>
<p>We can also take a look at only total acceleration in the three axes for all activities. The graph displayed happens to show subject 9. In this graph, the blue color is associated with the x-axis, the orange is associated with the y-axis, and green is associated with the z-axis. We can see those certain activities, such as activities 2 and 3 (walking upstairs and walking downstairs) have similar movements in the x-, y-, and z- direction which may make it difficult to classify. Regardless, each activity has it nuances that we can train.</p>
<p>Given that, we can train our data with the k-nearest neighbor, the multilayer perceptron, and the random forest classifier algorithms. When we do so, we find that k-nearest-neighbors with n = 1, gives us an accuracy score of 89.9%, multilayer perceptron gives us an accuracy score of 85.7%, and random forest classifier with 100 trees gives us an accuracy score of 84.7%. With that in mind, I can conclude that the K-Nearest Neighbors algorithm, with the number of neighbors equal to 1, was the best algorithm to train our HAR dataset.</p>
<p>However, it is important to understand that further experimentation is needed to definitively say that KNN is better than every other algorithm. All algorithms produce similar scores.</p>
<ol start="6" type="1">
<li>Concluding Discussion</li>
</ol>
<p>Through our experimentation, we found that KNN was better than multilayer perceptron and random forest classifier. However, more experimentation is needed. If we had more time, we could look at window data instead of snapshot data. We could also look at some other algorithms such as a support vector machine or other neural networks, such as a convolutional NN. It would also be important to look at other, more inclusive data sets and train the same algorithms on them as well. With that in mind, our project supports the research paper, Human Activity Recognition Using K-Nearest Neighbor Machine Learning Algorithm by Saeed Mohsen, Ahmed Elkaseer, and Steffen G. Scholz.</p>
<p>Our project definitely worked well. We were able to gather data and clean it up a little bit, though most of the tedious work was done for us. We were able to look at some graphs and figure out what question we wanted to answer. Our goal was to use a few predictive models on our data, and we were able to do that. We have well-documented and clean code as well. I would say that we were very successful!</p>
<ol start="7" type="1">
<li>Group Contribution</li>
</ol>
<p>I started by doing the literature review. I read a couple of papers and came up with experimentation. Zayn found the dataset that we worked with. Then, Zayn and I worked on loading in the dataset, which corresponds to the first three functions. I wrote the class_breakdown, data_for_subject, series, and plot_subject functions. I was able to output a plot, and Zayn helped edit the outputs, so they looked a little bit prettier. Zayn wrote the code for the second plot, which is a histogram of total acceleration in the x-axis, y-axis, and z-axis. I also wrote the code for the algorithms, and Mead helped put the code in a for loop. Finally, I wrote all of the embedded writing, describing our outputs and decision making.</p>
<ol start="8" type="1">
<li>Personal Reflection</li>
</ol>
<p>Through this project, I learned a lot about how people experiment, test, and find the most optimal algorithm or algorithms. People extensively test and experiment and then ultimately, come up with a conclusion. It feels very scientific, which is an experience I haven’t experienced in previous computer science classes.</p>
<p>I also feel that while my project could be a lot more in depth, I exceeded my own expectations with this project. I was able to be a great teammate and leader. I assigned different roles to different people. I also did a lot of work on this project. I researched about biases and social responsibility, learned a lot more about experimentation, and I practiced my teamwork and collaboration skills.</p>
<p>I will take a lot away with me. I learned a lot more about how experimentation works and how people come up with experiments. Most of the time, people attempt to prove or disprove a research paper. Other times people follow and recreate published research. I also will take away how to better work in a group. I learned how to organize better and how to better manage people.</p>
<p>Sources:</p>
<p><span class="citation" data-cites="Gupta_Gupta_Pathak_Jain_Rashidi_Suri_2022">Gupta et al. (<a href="#ref-Gupta_Gupta_Pathak_Jain_Rashidi_Suri_2022" role="doc-biblioref">2022</a>)</span></p>
<p><span class="citation" data-cites="Papaleonidas_Psathas_Iliadis_2021">Papaleonidas, Psathas, and Iliadis (<a href="#ref-Papaleonidas_Psathas_Iliadis_2021" role="doc-biblioref">2021</a>)</span></p>
<p><span class="citation" data-cites="Beddiar_Nini_Sabokrou_Hadid_2020">Beddiar et al. (<a href="#ref-Beddiar_Nini_Sabokrou_Hadid_2020" role="doc-biblioref">2020</a>)</span></p>
<p><span class="citation" data-cites="Reyes-Ortiz_Ghio_Oneto_Anguita_Parra_2013">Reyes-Ortiz et al. (<a href="#ref-Reyes-Ortiz_Ghio_Oneto_Anguita_Parra_2013" role="doc-biblioref">2013</a>)</span></p>
<p><span class="citation" data-cites="Mohsen_Elkaseer_Scholz_2021">Mohsen, Elkaseer, and Scholz (<a href="#ref-Mohsen_Elkaseer_Scholz_2021" role="doc-biblioref">2021</a>)</span></p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Beddiar_Nini_Sabokrou_Hadid_2020" class="csl-entry" role="doc-biblioentry">
Beddiar, Djamila Romaissa, Brahim Nini, Mohammad Sabokrou, and Abdenour Hadid. 2020. <span>“Vision-Based Human Activity Recognition: A Survey.”</span> <em>Multimedia Tools and Applications</em> 79 (41–42): 30509–55. <a href="https://doi.org/10.1007/s11042-020-09004-3">https://doi.org/10.1007/s11042-020-09004-3</a>.
</div>
<div id="ref-Gupta_Gupta_Pathak_Jain_Rashidi_Suri_2022" class="csl-entry" role="doc-biblioentry">
Gupta, Neha, Suneet K. Gupta, Rajesh K. Pathak, Vanita Jain, Parisa Rashidi, and Jasjit S. Suri. 2022. <span>“Human Activity Recognition in Artificial Intelligence Framework: A Narrative Review.”</span> <em>Artificial Intelligence Review</em> 55 (6): 4755–4808. <a href="https://doi.org/10.1007/s10462-021-10116-x">https://doi.org/10.1007/s10462-021-10116-x</a>.
</div>
<div id="ref-Mohsen_Elkaseer_Scholz_2021" class="csl-entry" role="doc-biblioentry">
Mohsen, Saeed, Ahmed Elkaseer, and Steffen G. Scholz. 2021. <span>“Human Activity Recognition Using k-Nearest Neighbor Machine Learning Algorithm.”</span> <em>SpringerLink</em>. Springer Singapore. <a href="https://link.springer.com/chapter/10.1007/978-981-16-6128-0_29">https://link.springer.com/chapter/10.1007/978-981-16-6128-0_29</a>.
</div>
<div id="ref-Papaleonidas_Psathas_Iliadis_2021" class="csl-entry" role="doc-biblioentry">
Papaleonidas, Antonios, Anastasios Panagiotis Psathas, and Lazaros Iliadis. 2021. <span>“High Accuracy Human Activity Recognition Using Machine Learning and Wearable Devices’ Raw Signals.”</span> <em>Journal of Information and Telecommunication</em> 6 (3): 237–53. <a href="https://doi.org/10.1080/24751839.2021.1987706">https://doi.org/10.1080/24751839.2021.1987706</a>.
</div>
<div id="ref-Reyes-Ortiz_Ghio_Oneto_Anguita_Parra_2013" class="csl-entry" role="doc-biblioentry">
Reyes-Ortiz, Jorge L, Alessandro Ghio, Luca Oneto, Davide Anguita, and Xavier Parra. 2013. <span>“Human Activity Recognition Using Smartphones Data Set.”</span> <em>UCI Machine Learning Repository: Human Activity Recognition Using Smartphones Data Set</em>. <a href="https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones">https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>