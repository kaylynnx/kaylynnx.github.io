{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83ee95b5-0c95-42b1-ac48-c9e303813976",
   "metadata": {},
   "source": [
    "---\n",
    "title: Hello Blog\n",
    "author: Kaylynn Xia\n",
    "date: '2023-01-10'\n",
    "image: \"image.jpg\"\n",
    "description: \"An example blog post illustrating the key techniques you'll need to demonstrate your learning in CSCI 0451.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef4db9-fb51-448b-85eb-f0eacf3282c5",
   "metadata": {},
   "source": [
    "Kaylynn Xia\n",
    "\n",
    "As long as humans have existed, technology has been a powerful force. From the wheel to running water to the computer, technology has aided our day-to-day lives. In modern-day technology, since the mid-1900s, technology has expanded past new inventions to more quantitative innovations. The generation of new phones and computers has become relatively stagnant, but the usage of phones and computers to aid our day-to-day lives has exploded. Scientists and businessmen created Machine Learning, more colloquially known as Artificial Intelligence. Artificial Intelligence has been displayed in movies such as Robocop or the recently released horror movie, Megan. These movies depict a universe in which Artificial Intelligence is self-aware and relatively human. While largely exaggerated, we have created machines to learn from data, hence why it is called Machine Learning. Given some historical data or a collection of current data, the machines can generate some sort of conclusion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2882f-f91d-4d4c-bbf3-027f804dbb9d",
   "metadata": {},
   "source": [
    "\n",
    "Machine Learning is used in the finance industry to uncover credit card fraud, make predictions about creditworthiness, and identify trends in the stock market (Ethical Implications of Bias in Machine Learning). Machine learning has been incredibly productive and helpful to provide important advances in health care and treatment decisions (Ethical Implications of Bias in Machine Learning). The criminal justice system is using machine learning to predict crime hotspots and recidivism rates (Ethical Implications of Bias in Machine Learning). AI and Machine learning has been increasingly widespread. They have been helpful in all aspects of our lives. But with all great innovations, there are unintended consequences. One of the unintended consequences is bias. What is bias? When we think of bias or fairness in a social, political, or economic sense, bias is defined as prejudice against one person, group, or thing in a way classified as ‘unfair’. Essentially, bias in an algorithmic sense, which we coin algorithmic bias, is very similar. Most researchers in this field define algorithmic bias differently. Nevertheless, “most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status” (Bias in Machine Learning – What is it Good For). What does this mean exactly? Well, bias in Machine Learning isn’t so black or white. Instead, the authors of Bias in Machine Learning found that biases can be organized into three distinct categories: “a biased world, data generation, and learning” (Bias in Machine Learning – What is it Good For). Within a biased world, bias may propagate through what people would refer to as historical bias. According to the Australian Human Rights Commission, “historical bias arises when the data used to train an AI system no longer accurately reflects the current reality” (Australian Human Rights Commission). Many obsolete data contain outdated language or ideals. For instance, when we looked at the Titanic data set, we see that sex contained a binary male or female. However, more up-to-date data would also consider those who are non-binary. Secondly, in the data generation category, they found five different sources of bias, including specification bias and inherited bias. Specification bias occurs when a potential independent component is excluded from the general model, resulting in a biased estimate of the coefficients. Inherited bias refers to the underlying assumptions that skew viewpoints and data. Finally, in the learning category, a way bias propagates is through inductive bias. The inductive bias of an algorithm is the set of assumptions the learner uses to predict outputs of given inputs that it has not encountered. That list is just a small, but widespread, list of ways bias presents itself in Machine Learning and algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784965d-2227-4493-a787-7658e823bd2f",
   "metadata": {},
   "source": [
    "These biases are reflected in the usage of algorithms in all of the examples I listed above. In finance, machine learning has been used to make predictions about creditworthiness. But what isn’t discussed is how minorities and underrepresented groups are hypothesized to have lower creditworthiness. As a result, it is harder for them to get loans, buy homes, and make investments. Machine learning has also made large advancements in healthcare. But for a while, women were given worse treatment than men because predictions were directed toward men. And POC was given worse treatment than white people because predictions and calculations were made specifically for white people. Machine learning was used in the criminal justice system to predict crime and recidivism rates, but the algorithm was so skewed that the algorithm targeted black individuals in black communities. It turned out that black individuals were given harsher sentences than white individuals for the same crime. But there are also other discriminatory algorithms. A study found that “Amazon’s recruitment tool, which produced AI-based recommendations that significantly favored men over women for technical jobs” (Questioning Racial and Gender Bias in AI-based Recommendations: Do Espoused National Cultural Values Matter?). \n",
    "\n",
    "Studying discrimination and prejudice in these algorithms, many researchers have concluded that these quantitative methods are incredibly problematic. One of those researchers is Arvind Narayanan. In his speech, The Limits of the Quantitative Approach to Discrimination, he argues that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Arvind Narayanan). While this is a strong statement, Arvind backs up this claim with strong evidence. In his speech, he talks about the practice of quantitative methods. More specifically, he talks about the usage of the null hypothesis. When studying racism, for instance, the null hypothesis is that there is no racism. Similarly, when studying any form of prejudice, the null hypothesis is that there is no prejudice. This is problematic because “baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (Arvind Narayanan). It’s problematic that the null hypothesis is that there is no discrimination because when there is no significant evidence that discrimination exists, people verify and justify the null hypothesis, thus seeing the status quo as something without flaws or issues. And “when researchers pick the null hypothesis on autopilot, mimicking what’s been done before, they are often oblivious to the fact that their choice has enormous normative significance” (Arvind Narayanan). There is also the issue with data. Often the data we use are snapshots from a day or a short period. But snapshots also lose people’s lived experiences because it is not recorded in the data. So, snapshots “frame discrimination as happening at discrete moments in time rather than encoded into the way that our institutions are designed” (Arvind Narayanan). In other words, the data doesn’t paint a holistic image, ignoring systematic and structural discrimination. Another problematic aspect of these quantitative methods is who produces the data. Usually, larger companies and organizations, both of which are arguably biased, gather data. So, “when companies are in control of producing data, they have simple ways of affecting the conclusions that are drawn by controlling which data are collected or released” (Arvind Narayanan). \n",
    "\n",
    "These sentiments by Arvind are echoed through other research papers. Dr. Alex Hanna from Harvard University, similar to Arvind, explains that AI research is supporting ‘bad markets’ for a lack of better words. Machine learning, algorithms, and AI are controlled by “large tech companies, elite universities, or specialty labs like Open AI or Anthropic which have these really big VC term sheets and are doing things which they consider to be general-purpose AI or something of that nature” (Dr. Alex Hanna). But the reason this is bad is that it doesn’t allow researchers to be independent or community focused. Instead, it supports a capitalistic society such as corporate uses, supporting their businesses, or directed towards military and security purposes. Thus Dr. Alex Hanna concludes that AI was created for negative reasons. As an example of this, Clearview AI, a large institution, created a widely known and used facial recognition tool for law enforcement. And ShotSpotter detects things like gunshots. These tools are designed to micromanage low-income communities and communities of color. Furthermore, she explains that a lot of the problem lies in the data. She explains that the data “does not provide the full story” (Dr. Alex Hanna). \n",
    "\n",
    "Adrienne Yapo and Joseph Weiss of Bentley University expressed similar disdain toward algorithms and machine learning. They suggested that the most significant issue is the “black box” secrecy behind the machine learning algorithms. In other words, the algorithms are not transparent. Why? “For-profit companies that produce these algorithms do not release the criteria and calculations behind the formulas” (Yapo and Weiss). Furthermore, at times the algorithms become so complex that understanding the formulas is extremely difficult. Therefore, since the algorithms are created by humans, “they inevitably – and often unconsciously – reflect societal values, biases, and discriminatory practices” (Yapo and Weiss). Ultimately, while Yapo and Weiss find that AI can be helpful at times, reformation is needed. Inclusivity and awareness of the ethical risks and complications are crucial to the design of AI to ensure that individuals are treated fairly. \n",
    "\n",
    "Technology and machine learning have come a long way. They have supported individuals in our day-to-day lives, making them important. However, the current usages of these algorithms and the foundations on which they lie are problematic. I, along with many other researchers and individuals, wouldn’t go as far as to say that the harms outweigh the benefits. There needs change and an ethical discussion, but these algorithms can still be helpful. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83780698-0772-48a2-97ff-4783b995583c",
   "metadata": {},
   "source": [
    "Sources:\n",
    "    \n",
    "https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf\n",
    "\n",
    "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1649&context=hicss-51\n",
    "\n",
    "https://arxiv.org/pdf/2004.00686.pdf \n",
    "\n",
    "https://link.springer.com/article/10.1007/s10796-021-10156-2\n",
    "\n",
    "https://www.sir.advancedleadership.harvard.edu/articles/understanding-gender-and-racial-bias-in-ai\n",
    "\n",
    "https://humanrights.gov.au/about/news/media-releases/infographic-historical-bias-ai-systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
